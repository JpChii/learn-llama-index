{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cj6rjGQNFwK"
      },
      "source": [
        "* Llama-index library Exploration\n",
        "* Creation of strucutred outputs from LLM outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od7cVzqftI4R"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm_jYQxIfER3"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhBe3s52hjF1",
        "outputId": "f048b11e-f54b-44ab-f0c7-1da383b8f8c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.6/187.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.5/252.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.6/250.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m271.6/271.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.2/113.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.9/275.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-llms-groq llama-index-embeddings-huggingface faiss-cpu \\\n",
        "llama-index-llms-huggingface llama-index-vector-stores-faiss llama-index-llms-gemini llama-index-embeddings-gemini\\\n",
        "llama-index-finetuning python-dotenv -q\n",
        "!pip install --upgrade sentence_transformers -q\n",
        "!python -m nltk.downloader punkt_tab -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6fS2jyufFxi"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GROAUAtss8cY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core.indices.vector_store import VectorStoreIndex\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from llama_index.core.storage.storage_context import StorageContext\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Settings\n",
        "from llama_index.core import load_index_from_storage\n",
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.embeddings.gemini import GeminiEmbedding\n",
        "from llama_index.finetuning.embeddings.common import generate_qa_embedding_pairs, EmbeddingQAFinetuneDataset\n",
        "# Import HitRate and MRR from llama_index\n",
        "from llama_index.core.evaluation.retrieval.metrics import HitRate, MRR\n",
        "# RetrieverEvaluator\n",
        "from llama_index.core.evaluation import RetrieverEvaluator\n",
        "import numpy as np\n",
        "import faiss\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEGIICGUfI3a"
      },
      "source": [
        "## Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OVrlkstxtyZn"
      },
      "outputs": [],
      "source": [
        "# Variables\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"/content/drive/MyDrive/Colab Notebooks/env\")\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "INDEX_DIR = \"/content/drive/MyDrive/Colab Notebooks/indexes\"\n",
        "LLAMA_INDEX_DIR = \"/content/drive/MyDrive/Colab Notebooks/llama-index\"\n",
        "EXP_DIR = os.path.join(LLAMA_INDEX_DIR, \"exp1\")\n",
        "# Functionize the index directory creation and clean up\n",
        "INDEX_FAISS_PAUL_GRAHAM = os.path.join(INDEX_DIR, \"faiss_hnsw_paul_graham\")\n",
        "INDEX_FAISS_PAUL_GRAHAM_L2 = os.path.join(INDEX_DIR, \"faiss_paul_graham_l2\")\n",
        "INDEX_FAISS_PAUL_GRAHAM_L2_v2 = os.path.join(INDEX_DIR, \"faiss_paul_graham_l2_v2\")\n",
        "os.makedirs(INDEX_FAISS_PAUL_GRAHAM, exist_ok=True)\n",
        "os.makedirs(INDEX_FAISS_PAUL_GRAHAM_L2, exist_ok=True)\n",
        "os.makedirs(INDEX_FAISS_PAUL_GRAHAM_L2_v2, exist_ok=True)\n",
        "os.makedirs(EXP_DIR, exist_ok=True) # Diretory to store all artifacts(nodes, indexes, datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93_GmZXIt1Up"
      },
      "source": [
        "## Download Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "14c2cf595be74112998e66d035eaaae2",
            "17a7a7126cfe40a9b1e3f8ebd6a1daba",
            "ff6da244afd340fd81187f82e259472f",
            "2050d6f1d9164657a14973c625092670",
            "e581e99ee084419295ab52b77757c6c7",
            "9140c0a56591430799f54521bb6b4338",
            "88b5a644b77a46ea95f6646621a26d2f",
            "e89cb216150c4bbe9a8445f8be61c39e",
            "8f5a326570ca4f099f8362297810ead6",
            "64ce9dcea9d94f9ca27d8842f67850e2",
            "d26cd61a697d42d5857c088c222c9b98"
          ]
        },
        "id": "HqglxQShjC65",
        "outputId": "f501a1b7-a014-4ec4-a79d-963dec35dc4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14c2cf595be74112998e66d035eaaae2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "embed_model = HuggingFaceEmbedding(model_name=\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\")\n",
        "# llm = HuggingFaceLLM(model_name=\"HuggingFaceTB/SmolLM-1.7B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SPSeZ_vO47y7"
      },
      "outputs": [],
      "source": [
        "llm_gemini = Gemini(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    api_key=GEMINI_API_KEY,\n",
        ")\n",
        "embed_model_gemini = GeminiEmbedding(\n",
        "    model=\"models/text-embedding-004\",\n",
        "    api_key=GEMINI_API_KEY,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1_EbnbwpY7u"
      },
      "source": [
        "# Implementing and Optimizing a RAG pipeline with LLamaIndex\n",
        "\n",
        "1. Dataset - Paul Graham Essay. (Using a small dataset for compute limitations on embeddings creations)\n",
        "2.Embedding model - `gte-Qwen2-1.5B-instruct`\n",
        "3. LLM - `SmolLM-1.7B`\n",
        "\n",
        "Using small models for Embeddings, LLM due to compute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x-TpuuAZ96K"
      },
      "source": [
        "## Simple RAG Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW-JVJClK7i4"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UdQXorCaHsl",
        "outputId": "7a8a45cd-5ec7-4af3-99ca-c042a5c67624"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-01-28 05:05:06--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘data/paul_graham_essay.txt’\n",
            "\n",
            "data/paul_graham_es 100%[===================>]  73.28K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-01-28 05:05:07 (2.32 MB/s) - ‘data/paul_graham_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham_essay.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbTsv6kCaJBg"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"./data/\").load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpSfqQiYaL6m",
        "outputId": "41aebdcd-d506-4c8e-8cc6-d312d655d691"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of documents: 1\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of documents: {len(documents)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mH4z4u1aPn8",
        "outputId": "f58b7977-4f5b-48f7-b340-7f1950c220e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['id_', 'embedding', 'metadata', 'excluded_embed_metadata_keys', 'excluded_llm_metadata_keys', 'relationships', 'metadata_template', 'metadata_separator', 'text_resource', 'image_resource', 'audio_resource', 'video_resource', 'text_template'])\n",
            "--------------------------------------------------\n",
            "{'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-23', 'last_modified_date': '2025-01-23'}\n"
          ]
        }
      ],
      "source": [
        "# Loads the text data, creates an id_, metadata has file details(path, name, type, size, creation date, modification date etc.)\n",
        "# text_resource has the main text data\n",
        "# SimpleDirectoryReader is implemented to load different datatypes(txt, html, markdown etc).\n",
        "print(documents[0].__dict__.keys())\n",
        "print(\"-\"*50)\n",
        "print(documents[0].__dict__[\"metadata\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8l1dZftactA"
      },
      "source": [
        "### Ingestion\n",
        "\n",
        "To ingest data with LlamaIndex, we've to create nodes out of Documents with node_parsers.\n",
        "\n",
        "LlamaIndex provides variety of Splitters and Parsers. [Docs](https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/sentence_window/).\n",
        "\n",
        "Here, we'll use a simple SentenceSplitter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xpxpf_3GeqBV"
      },
      "outputs": [],
      "source": [
        "sentences = sent_tokenize(documents[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMDZbm20e9uL",
        "outputId": "143cd025-cca0-401b-d46b-73f7d677b1fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences with nltk tokenizer: 756\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of sentences with nltk tokenizer: {len(sentences)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voWLL8Z9fO-U",
        "outputId": "5bd43c53-5926-4779-8250-862daf5d54de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of words: 75011\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total number of words: {len(documents[0].text)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjI_0DuIbnX1"
      },
      "outputs": [],
      "source": [
        "sentence_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEVIdXF9fokb"
      },
      "outputs": [],
      "source": [
        "# Create nodes/chunks\n",
        "nodes = sentence_splitter.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bsGlxurfu5C",
        "outputId": "b6194116-e30b-4780-df8e-b942a7eafdb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nodes: 37\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of nodes: {nodes.__len__()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "AHycpoNdfzGZ",
        "outputId": "be81212b-5423-4d7b-9220-dce363c30991"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# See distirbution of chunks\n",
        "chunk_sizes = [len(word_tokenize(node.text)) for node in nodes]\n",
        "plt.hist(chunk_sizes)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBuV4DEKgLit",
        "outputId": "b4e6aad3-1982-424e-ca29-ac2013771a26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average chunk size: 437.97297297297297\n"
          ]
        }
      ],
      "source": [
        "print(f\"Average chunk size: {np.mean(chunk_sizes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Jy5iHbGf2cp",
        "outputId": "b82b389f-e965-4817-fc3b-5e8e14cb0926"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "95th percentile: 467.0\n"
          ]
        }
      ],
      "source": [
        "print(f\"95th percentile: {np.percentile(chunk_sizes, 95)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXRcUU5viWw4"
      },
      "source": [
        "One chunk is outlier(150) might be the last text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-F8QjQxisi0"
      },
      "source": [
        "### Indexing\n",
        "\n",
        "Nodes/Chunks are created. Let's create embeddings and the index on top of it.\n",
        "We'll use an In-Memory vector store - Faiss for this implementaion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEcBonpqj5Pt",
        "outputId": "cb0beafe-e451-4e2c-e78a-033e0a6e3e48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embed_dim = embed_model.get_text_embedding(\"Hello\").__len__()\n",
        "embed_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQDd2mndi7yV"
      },
      "outputs": [],
      "source": [
        "M = 4 # Number of neighbours per vertex\n",
        "faiss_index = faiss.IndexHNSWFlat(embed_dim, M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maZw33FXjw3E",
        "outputId": "8cb36611-11f1-4a24-e5ab-9c497e683c4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "faiss_index.hnsw.max_level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3oyqM4hFdco",
        "outputId": "af8457ba-c675-4cd6-8441-e1d005294508"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INDEX_EXISTS = len(os.listdir(INDEX_FAISS_PAUL_GRAHAM)) > 0\n",
        "INDEX_EXISTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "bba2b8778f7940b0853f9a33efcf947d",
            "b0187ea479de46588f5e9c98b8691987",
            "209b3bb6278e4848b8b981b70c55d75a",
            "f8b67a2eb2d44ef28ceae2d0de6f66ad",
            "b0a56d0fd46f4fb88c7ac073516ffd50",
            "329f46150ee840d691d2a0bce9d24d6b",
            "98d9afff57044c0e9e89ed2c359cf7d5",
            "00c66838a1a948cca84b86a3040e0030",
            "12c0faf254aa46f5b4c15967b66cf6bb",
            "f8367cd7025648ee85e961c9548e9444",
            "4e64b0e4632d42ceb1e94b2c35143143"
          ]
        },
        "id": "6RnWRr3p06Sc",
        "outputId": "99caedc7-f28a-481c-ac66-cf31e4338781"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bba2b8778f7940b0853f9a33efcf947d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating embeddings:   0%|          | 0/37 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=vector_store,\n",
        ")\n",
        "\n",
        "NEW_INDEX = True # Optional parameter to override existing\n",
        "if not INDEX_EXISTS or NEW_INDEX:\n",
        "  vector_index = VectorStoreIndex(\n",
        "      nodes,\n",
        "      storage_context=storage_context,\n",
        "      embed_model=embed_model,\n",
        "      show_progress=True,\n",
        "\n",
        "  )\n",
        "  vector_index.storage_context.persist(INDEX_FAISS_PAUL_GRAHAM) # Store index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB0zSdQF4DZO",
        "outputId": "3756e9d0-848c-4089-d40d-e518f24e1a69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "False or True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asb126322Fih"
      },
      "source": [
        "HNSW Faiss Index with gte-qwen-1.5-Instruct embeddings takes around 20minutes.\n",
        "\n",
        "* Index Dimension: 1356\n",
        "* HNSW Max Neighbor pers vertex: 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7ATXP5Fsorc"
      },
      "outputs": [],
      "source": [
        "# Load index\n",
        "Settings.embed_model = embed_model\n",
        "if INDEX_EXISTS:\n",
        "  loaded_vector_store = FaissVectorStore.from_persist_dir(persist_dir=INDEX_FAISS_PAUL_GRAHAM)\n",
        "  loaded_storage_context = StorageContext.from_defaults(\n",
        "      vector_store=loaded_vector_store,\n",
        "      persist_dir=INDEX_FAISS_PAUL_GRAHAM, # Required to load index from storage.\n",
        "  )\n",
        "  loaded_index = load_index_from_storage(storage_context=loaded_storage_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3fnZtvo8mYu"
      },
      "source": [
        "### Querying\n",
        "\n",
        "We can use the index as query engine. QueryEngine in LlamaIndex enables us to query the created index with Natural Language and generate answers on the query. What lies under the QueryEngine is an Retriever and Generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17TceoJs9LUL",
        "outputId": "4b27d516-e695-46d4-8ede-0e22aecd8ecb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "�???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n"
          ]
        }
      ],
      "source": [
        "# simple querying\n",
        "Settings.llm = llm\n",
        "query_engine = vector_index.as_query_engine(top_k=5)\n",
        "response = query_engine.query(\"What did the author do growing up?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehLXb8wVAa6g"
      },
      "source": [
        "top_k=5, chunk_size=512, 1.7BLLM, 1536dim, 2CPUs and the generation nearly takes close to 10minutes(1st iter), 43minutes(2nd iter) for just ?'s lol.\n",
        "\n",
        "Generation time is not consistent.\n",
        "\n",
        "This is the limitations with limited compute and LargeLanguage Models. If compute and time is available go with Open-Source models. Application building is valued higher irrespective of cost and transperancy opt for Maintained models.\n",
        "\n",
        "To get more visisblity, let's split the retriever and generation for evaluation purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WX_enXrD9UTS"
      },
      "outputs": [],
      "source": [
        "retriever = vector_index.as_retriever(top_k=5)\n",
        "top_k_nodes = retriever.retrieve(\"What did the author do growing up?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4T7gtQo-2u2"
      },
      "source": [
        "`retriever.retrieve` calls [query()](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/vector_stores/simple.py). This finally calls the [get_topk_embeddings()](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/indices/query/embedding_utils.py#L11), which calls [similarity()](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/base/embeddings/base.py). This by default uses `cosine similarity`.\n",
        "\n",
        "Other similarity methods available are:\n",
        "1. Euclidean distance.\n",
        "2. Dot Product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWoLMaaGC-bW"
      },
      "source": [
        "Retriever returns `NodeWithScore` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzx0xZtODciT",
        "outputId": "107407ad-0032-46c1-aec5-5cd5a54c4496"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(top_k_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owdKG-IgBCRB",
        "outputId": "4c7119c7-8aa0-4c85-f5fa-7019171f3752"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['node', 'score'])"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "top_k_nodes[0].__dict__.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ur2Uw0iLDCDB",
        "outputId": "a8f86a0b-5982-4481-f0a2-129b6d182d0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[1.1232140064239502, 1.1505811214447021]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores = [node.score for node in top_k_nodes]\n",
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaXmzKhABD08"
      },
      "source": [
        "Default Similarity method is cosine but the scores are more than 1. Cosine Similarity Score ranges between 0 and 1. Why? Let's reverify this again with Building a retriever from scratch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDAdvBdGEDBP",
        "outputId": "2fd1bd5c-9d8c-4d00-aaf8-fa6a435ab4d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TextNode(id_='cff427a0-1a28-456f-a178-6c04211b0caa', embedding=None, metadata={'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-21', 'last_modified_date': '2025-01-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3c1a86b4-8077-410b-b50c-d460508cc38d', node_type='4', metadata={'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-21', 'last_modified_date': '2025-01-21'}, hash='0c3c3f46cac874b495d944dfc4b920f6b68817dbbb1699ecc955d1fafb2bf87b'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='ee18e1c0-5bcb-4005-91f5-fa22cf4535cb', node_type='1', metadata={'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-21', 'last_modified_date': '2025-01-21'}, hash='3eae173bbf97ba0a428f4214be89f19d06266adfe469623b22106c52e1195700'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='76302a56-7d26-4263-9baf-a7079479d505', node_type='1', metadata={}, hash='c0d6eec84f92ccdad207f544d047fc5fd9d1d9fd57ee03d59760ab17e1404dd0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"In everyday life it would be distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and when you do there's a lot to see. You can still be noticing new things after days of trying to paint something people usually take for granted, just as you can after days of trying to write an essay about something people usually take for granted.\\n\\nThis is not the only way to paint. I'm not 100% sure it's even a good way to paint. But it seemed a good enough bet to be worth trying.\\n\\nOur teacher, professor Ulivi, was a nice guy. He could see I worked hard, and gave me a good grade, which he wrote down in a sort of passport each student had. But the Accademia wasn't teaching me anything except Italian, and my money was running out, so at the end of the first year I went back to the US.\\n\\nI wanted to go back to RISD, but I was now broke and RISD was very expensive, so I decided to get a job for a year and then return to RISD the next fall. I got one at a company called Interleaf, which made software for creating documents. You mean like Microsoft Word? Exactly. That was how I learned that low end software tends to eat high end software. But Interleaf still had a few years to live yet. [5]\\n\\nInterleaf had done something pretty bold. Inspired by Emacs, they'd added a scripting language, and even made the scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing I've had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was the thinnest icing on a giant C cake, and since I didn't know C and didn't want to learn it, I never understood most of the software. Plus I was terribly irresponsible. This was back when a programming job meant showing up every day during certain working hours. That seemed unnatural to me, and on this point the rest of the world is coming around to my way of thinking, but at the time it caused a lot of friction.\", mimetype='text/plain', start_char_idx=16574, end_char_idx=18595, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}')"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "top_k_nodes[0].node"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Acem95MpcJZq"
      },
      "source": [
        "## Retriever Evaluation\n",
        "\n",
        "Now, we've a RAG pipeline. Next step is to evaluate it. We don't have a dataset to evaluate retrieval. Let's create a Synthetic Dataset with chunks.\n",
        "\n",
        "We'll use Gemini going Forward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rT8kFO8OsE8"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "This section covers:\n",
        "1. qa dataset creation\n",
        "2. Save and Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z57-aytJndkc"
      },
      "outputs": [],
      "source": [
        "Settings.llm = llm_gemini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSEdNpeqohza"
      },
      "source": [
        "[`generate_context_question_pairs()`](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/llama_dataset/legacy/embedding.py) from llama-index, we can generate questions. Prompt used:\n",
        "\n",
        "```\n",
        "DEFAULT_QA_GENERATE_PROMPT_TMPL = \"\"\"\\\n",
        "Context information is below.\n",
        "\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "\n",
        "Given the context information and not prior knowledge.\n",
        "generate only questions based on the below query.\n",
        "\n",
        "You are a Teacher/ Professor. Your task is to setup \\\n",
        "{num_questions_per_chunk} questions for an upcoming \\\n",
        "quiz/examination. The questions should be diverse in nature \\\n",
        "across the document. Restrict the questions to the \\\n",
        "context information provided.\"\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "Generates questions, Extracts question with regex `re.sub(r\"^\\d+[\\).\\s]\", \"\", question)`. Evaluates only the number of questions parameter is matched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ICP89eYpior",
        "outputId": "2ee89904-ff49-4574-af0b-d8fe924a02b6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_keys(['nodes', 'llm', 'qa_generate_prompt_tmpl', 'num_questions_per_chunk', 'retry_limit', 'on_failure', 'save_every', 'output_path', 'verbose'])"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import inspect\n",
        "inspect.signature(generate_qa_embedding_pairs).parameters.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3KwToZFvCHSd",
        "outputId": "87274cbc-2db1-412f-9af8-3638b89ae663"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 16/37 [00:26<00:33,  1.60s/it]WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1037.43ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 1/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1037.43ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1163.79ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 2/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1163.79ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 46%|████▌     | 17/37 [00:30<00:46,  2.33s/it]WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1224.17ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 1/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1224.17ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1164.25ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 2/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1164.25ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1138.75ms\n",
            "\r 49%|████▊     | 18/37 [00:34<00:51,  2.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 3/3...\n",
            "Skipping node eb688395-95d4-409f-98ae-bc090daac9cc after 3 retries.\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1138.75ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1163.97ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 1/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1163.97ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 990.36ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 2/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 990.36ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1062.86ms\n",
            " 51%|█████▏    | 19/37 [00:37<00:51,  2.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 3/3...\n",
            "Skipping node 2335b6bb-27c6-47c0-a78a-86df0828fcc3 after 3 retries.\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1062.86ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1240.18ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 1/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1240.18ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1164.88ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 2/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1164.88ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1214.83ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 3/3...429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1214.83ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 54%|█████▍    | 20/37 [00:41<00:52,  3.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Skipping node e7528987-0b46-4a12-883b-333b63e85724 after 3 retries.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1189.83ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 1/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1189.83ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1343.64ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 2/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1343.64ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r 57%|█████▋    | 21/37 [00:45<00:53,  3.32s/it]WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1300.06ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 3/3...\n",
            "Skipping node e4a1dac8-fdcc-44d0-86e9-acafc6b8c346 after 3 retries.\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1300.06ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1188.01ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 1/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1188.01ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1265.42ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 2/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1265.42ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1442.01ms\n",
            "\r 59%|█████▉    | 22/37 [00:49<00:52,  3.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 3/3...\n",
            "Skipping node 5bd9c934-709c-45fc-8db7-2d3190038f2e after 3 retries.\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1442.01ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1240.25ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 1/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1240.25ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1340.93ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 2/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1340.93ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1290.33ms\n",
            " 62%|██████▏   | 23/37 [00:53<00:50,  3.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 3/3...\n",
            "Skipping node 0a1b5ec7-26ea-47f4-8ff4-23045a95e236 after 3 retries.\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1290.33ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1189.30ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 1/3...429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1189.30ms\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1113.71ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 2/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1113.71ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1087.66ms\n",
            "\r 65%|██████▍   | 24/37 [00:56<00:46,  3.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 3/3...\n",
            "Skipping node e1d90b6f-3fe8-4f9c-93eb-50f15cb41206 after 3 retries.\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1087.66ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1168.66ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 1/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1168.66ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1190.47ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 2/3...\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1190.47ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1215.06ms\n",
            "\r 68%|██████▊   | 25/37 [01:00<00:42,  3.58s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error querying LLM: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).. Retrying 3/3...\n",
            "Skipping node 719b355e-f219-4e3d-87ad-f4bc1f48c376 after 3 retries.\n",
            "429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1215.06ms\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 37/37 [01:20<00:00,  2.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate QA pairs\n",
        "qa_dataset = generate_qa_embedding_pairs(nodes=nodes, llm=llm_gemini, num_questions_per_chunk=1, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "HTRF-9DWOM_m",
        "outputId": "cf36de56-85d1-45a3-f07b-68a2641491ef"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>llama_index.finetuning.embeddings.common.EmbeddingQAFinetuneDataset</b><br/>def __init__(self, /, **data: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/llama_index/finetuning/embeddings/common.py</a>Embedding QA Finetuning Dataset.\n",
              "\n",
              "Args:\n",
              "    queries (Dict[str, str]): Dict id -&gt; query.\n",
              "    corpus (Dict[str, str]): Dict id -&gt; string.\n",
              "    relevant_docs (Dict[str, List[str]]): Dict query id -&gt; list of doc ids.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 12);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "llama_index.finetuning.embeddings.common.EmbeddingQAFinetuneDataset"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(qa_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyTjfczHNaXm"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "pickle.dump(qa_dataset, open(\"/content/drive/MyDrive/Colab Notebooks/datasets/li-qa-dataset-pg.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngLvzqDmN-mZ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "qa_loaded = pickle.load(open(\"/content/drive/MyDrive/Colab Notebooks/datasets/li-qa-dataset-pg.pkl\", \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPGssuxWOH5Z"
      },
      "outputs": [],
      "source": [
        "qa_loaded_ds = EmbeddingQAFinetuneDataset.model_validate(qa_loaded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A76uv1h-Cwqg",
        "outputId": "a35c1a35-8d6a-4074-bcb6-3867561bed17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['queries', 'corpus', 'relevant_docs', 'mode'])"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa_dataset.__dict__.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpeAGwfUDM3d",
        "outputId": "1f8e3d57-a8e5-4ca0-c4b3-fddacc6d0bd9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('a4720f13-80d2-44dc-a0ed-ab441f3a2ac9',\n",
              " \"What limitations of the IBM 1401 computer hampered the author's early programming efforts, and what specific example illustrates this limitation?\")"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_id = list(qa_dataset.__dict__[\"queries\"])[0]\n",
        "query_id, qa_dataset.__dict__.get(\"queries\").get(query_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny2o6bMuFsh1",
        "outputId": "431a58bd-eac9-4acd-da4f-890c97980ef7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['124db005-422f-4bbb-901f-3b5dea279c4b']"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "node_id = qa_dataset.__dict__[\"relevant_docs\"][query_id]\n",
        "node_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "p7X-ulQsGFtF",
        "outputId": "205dbd7b-7c64-4c60-9905-e3db9d807cbb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping.'"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# node_ids to text map\n",
        "ids_to_text = {node.id_: node.text for node in nodes}\n",
        "ids_to_text.get(node_id[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AtGlKY7NQqv"
      },
      "source": [
        "Ouptut from `generate_qa_embedding_pairs()`, returns:\n",
        "1. queries --> Dict lookup {uuid: question_str}\n",
        "2. relevant_docs --> Dict lookup {query_uuid: [node_ids]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiPxXhm4Ge6N"
      },
      "outputs": [],
      "source": [
        "eval_response = llm_gemini.complete(\n",
        "    f\"\"\"\n",
        "    Given a question and context, evaluate how relevant it is to the context. Given a Flag on Relevancy. True or False are the flags.\n",
        "    Return a dict ouptut with rating, explaination.\n",
        "    The dict output has to be clean without any new lines\n",
        "    Question: {qa_dataset.__dict__.get(\"queries\").get(query_id)}\n",
        "    Context: {ids_to_text.get(node_id[0])}\n",
        "    Answer:\n",
        "    \"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "MblMrY2zHDoY",
        "outputId": "6fb15842-7bb8-44a8-fe5e-0631172f3e87"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'```json\\n{\"rating\": true, \"explanation\": \"The context explicitly discusses the author\\'s early programming experiences on an IBM 1401, mentioning the limitations of using punch cards as the sole input method and the lack of interactive feedback.  The author\\'s inability to find suitable data on punch cards and the resulting inability to create interesting programs directly addresses the question\\'s prompt for limitations and a specific example. The anecdote about the program that didn\\'t terminate also illustrates a limitation of the system\\'s lack of time-sharing.\"}\\n```\\n'"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "eval_response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LSdt5ueOFQ-"
      },
      "source": [
        "### Structured outputs from LLM output\n",
        "\n",
        "Below cells gives an intution of abstraction in llama-index, langchain and other frameworks in creating strucutred outputs with LLM text.\n",
        "\n",
        "Regex is power!!🔥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "IXi25BlMJ0em",
        "outputId": "2fc0d8ac-092f-4363-d988-049f27419ac0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\"rating\": true, \"explanation\": \"The context explicitly discusses the author\\'s early programming experiences on an IBM 1401, mentioning the limitations of using punch cards as the sole input method and the lack of interactive feedback.  The author\\'s inability to find suitable data on punch cards and the resulting inability to create interesting programs directly addresses the question\\'s prompt for limitations and a specific example. The anecdote about the program that didn\\'t terminate also illustrates a limitation of the system\\'s lack of time-sharing.\"}'"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Regex from langchain to extract the JSON output\n",
        "import re\n",
        "json_ = re.search(r\"\\{.*\\}\", eval_response.text.strip(), re.MULTILINE | re.IGNORECASE | re.DOTALL)\n",
        "json_.group()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jnf-zBLrMuOx"
      },
      "source": [
        "There is some conversion or cleaning done and something is like below is ran to create strucutred outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpfdSpHzJ_w3",
        "outputId": "36a300fb-88df-4b7c-abce-8cd9fe3893bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sample(rating=True, explanation=\"The context explicitly discusses the author's early programming experiences on an IBM 1401, mentioning the limitations of using punch cards as the sole input method and the lack of interactive feedback.  The author's inability to find suitable data on punch cards and the resulting inability to create interesting programs directly addresses the question's prompt for limitations and a specific example. The anecdote about the program that didn't terminate also illustrates a limitation of the system's lack of time-sharing.\")"
            ]
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "class Sample(BaseModel):\n",
        "  rating: bool\n",
        "  explanation: str\n",
        "\n",
        "sample = Sample.model_validate({\"rating\": True, \"explanation\": \"The context explicitly discusses the author\\'s early programming experiences on an IBM 1401, mentioning the limitations of using punch cards as the sole input method and the lack of interactive feedback.  The author\\'s inability to find suitable data on punch cards and the resulting inability to create interesting programs directly addresses the question\\'s prompt for limitations and a specific example. The anecdote about the program that didn\\'t terminate also illustrates a limitation of the system\\'s lack of time-sharing.\"})\n",
        "sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2eAqCjdHLpy"
      },
      "source": [
        "We can run the above prompt and stor the relevancy of question to context and store them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHiM6ln3Dvns"
      },
      "source": [
        "The chunks are really big, Let's check a single question if it's relevant to chunk.(This should be good but checking or creating evaluation datasets is good as it determines your performance of the application or pipeline)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe35iwtvEHtU",
        "outputId": "a4ba9461-98ca-443e-a1e6-acc9e46971ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id_': '124db005-422f-4bbb-901f-3b5dea279c4b',\n",
              " 'embedding': None,\n",
              " 'metadata': {'file_path': '/content/data/paul_graham_essay.txt',\n",
              "  'file_name': 'paul_graham_essay.txt',\n",
              "  'file_type': 'text/plain',\n",
              "  'file_size': 75042,\n",
              "  'creation_date': '2025-01-21',\n",
              "  'last_modified_date': '2025-01-21'},\n",
              " 'excluded_embed_metadata_keys': ['file_name',\n",
              "  'file_type',\n",
              "  'file_size',\n",
              "  'creation_date',\n",
              "  'last_modified_date',\n",
              "  'last_accessed_date'],\n",
              " 'excluded_llm_metadata_keys': ['file_name',\n",
              "  'file_type',\n",
              "  'file_size',\n",
              "  'creation_date',\n",
              "  'last_modified_date',\n",
              "  'last_accessed_date'],\n",
              " 'relationships': {<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3c1a86b4-8077-410b-b50c-d460508cc38d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-21', 'last_modified_date': '2025-01-21'}, hash='0c3c3f46cac874b495d944dfc4b920f6b68817dbbb1699ecc955d1fafb2bf87b'),\n",
              "  <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='5fd078d3-a893-44ef-9673-ca0fe098360f', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='f2452c76b5c459e5d8eba44f3aef55c05c3d73826534e86275b8355d7e2ff08a')},\n",
              " 'metadata_template': '{key}: {value}',\n",
              " 'metadata_separator': '\\n',\n",
              " 'text': 'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping.',\n",
              " 'mimetype': 'text/plain',\n",
              " 'start_char_idx': 0,\n",
              " 'end_char_idx': 2184,\n",
              " 'metadata_seperator': '\\n',\n",
              " 'text_template': '{metadata_str}\\n\\n{content}'}"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vars(nodes[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PHkkdABuEJt9",
        "outputId": "ac126737-1024-4594-eabc-04d770b65248"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'5fd078d3-a893-44ef-9673-ca0fe098360f'"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nodes[1].id_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnA3ZN9SEVM5"
      },
      "source": [
        "### Retriever\n",
        "\n",
        "We'll use two metrics to evaluate the retrieved docs:\n",
        "1. Hit Rate: How often relevant context is present in top_k_docs\n",
        "2. Mean Reciprocal Rank(MRR): In which top_k docs relevant context is found(whether it's 1st, 2nd or 3rd and so on.). This score decreases as it goes down the list of docs.\n",
        "\n",
        "[Metrics Implementation](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/evaluation/retrieval/metrics.py). We can get a granular and better score by importing metrics from here and passing the metrics directly to `RetrieverEvaluator` instead of using `RetrieverEvaluator.from_metric_names()`\n",
        "\n",
        "We already have the dataset with Syntheic Question ids mapped with their respective relevant document ids in `qa_dataset`.\n",
        "\n",
        "`RetrieverEvaluator` evaluates `qa_dataset` on the metrics passed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbqVfoCU3G8W"
      },
      "source": [
        "#### Loaded Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "pomsgxuBR_H9",
        "outputId": "e1fb244c-26ff-42fb-bcc2-8d0de87b87bc"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Retrieved ids and expected ids must be provided",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-87445a241ab2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Provides only async evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maevaluate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqa_loaded_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/evaluation/retrieval/base.py\u001b[0m in \u001b[0;36maevaluate_dataset\u001b[0;34m(self, dataset, workers, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mtqdm_asyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresponse_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresponse_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0meval_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/evaluation/retrieval/base.py\u001b[0m in \u001b[0;36meval_worker\u001b[0;34m(query, expected_ids, mode)\u001b[0m\n\u001b[1;32m    180\u001b[0m         ) -> RetrievalEvalResult:\n\u001b[1;32m    181\u001b[0m             \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msemaphore\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mresponse_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/evaluation/retrieval/base.py\u001b[0m in \u001b[0;36maevaluate\u001b[0;34m(self, query, expected_ids, expected_texts, mode, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mmetric_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             eval_result = metric.compute(\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrieved_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpected_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrieved_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/evaluation/retrieval/metrics.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, query, expected_ids, retrieved_ids, expected_texts, retrieved_texts, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexpected_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         ):\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Retrieved ids and expected ids must be provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_granular_hit_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Retrieved ids and expected ids must be provided"
          ]
        }
      ],
      "source": [
        "# Evaluate for a single k\n",
        "retriever = loaded_index.as_retriever(top_k=2)\n",
        "# Creates a retriever with implementation of respective metrics.\n",
        "evaluator = RetrieverEvaluator.from_metric_names(\n",
        "    retriever=retriever,\n",
        "    metric_names=[\"hit_rate\", \"mrr\"],\n",
        ")\n",
        "# Provides only async evaluation\n",
        "results = await evaluator.aevaluate_dataset(dataset=qa_loaded_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TwcI2FTPkxl"
      },
      "source": [
        "Why the error?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQhlXijNRmCC",
        "outputId": "5797f68e-47ad-4a08-8cdc-c9502518fba2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[NodeWithScore(node=TextNode(id_='3e94e0c4-7ba9-4a11-ad4f-80fb2d55cc09', embedding=None, metadata={'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ad6fee52-f00f-4224-97a5-d23758b3a213', node_type='4', metadata={'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}, hash='0c3c3f46cac874b495d944dfc4b920f6b68817dbbb1699ecc955d1fafb2bf87b'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c00aefd3-4d0a-45a8-8784-b7c6a99c739d', node_type='1', metadata={'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}, hash='928bba6f9657c9dfac691a9697530ff62ed4c976c6f8d5fc469ed86fc7e6e419'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='af572889-0175-42b9-a622-29574c176917', node_type='1', metadata={}, hash='c0d6eec84f92ccdad207f544d047fc5fd9d1d9fd57ee03d59760ab17e1404dd0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"In everyday life it would be distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and when you do there's a lot to see. You can still be noticing new things after days of trying to paint something people usually take for granted, just as you can after days of trying to write an essay about something people usually take for granted.\\n\\nThis is not the only way to paint. I'm not 100% sure it's even a good way to paint. But it seemed a good enough bet to be worth trying.\\n\\nOur teacher, professor Ulivi, was a nice guy. He could see I worked hard, and gave me a good grade, which he wrote down in a sort of passport each student had. But the Accademia wasn't teaching me anything except Italian, and my money was running out, so at the end of the first year I went back to the US.\\n\\nI wanted to go back to RISD, but I was now broke and RISD was very expensive, so I decided to get a job for a year and then return to RISD the next fall. I got one at a company called Interleaf, which made software for creating documents. You mean like Microsoft Word? Exactly. That was how I learned that low end software tends to eat high end software. But Interleaf still had a few years to live yet. [5]\\n\\nInterleaf had done something pretty bold. Inspired by Emacs, they'd added a scripting language, and even made the scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing I've had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was the thinnest icing on a giant C cake, and since I didn't know C and didn't want to learn it, I never understood most of the software. Plus I was terribly irresponsible. This was back when a programming job meant showing up every day during certain working hours. That seemed unnatural to me, and on this point the rest of the world is coming around to my way of thinking, but at the time it caused a lot of friction.\", mimetype='text/plain', start_char_idx=16574, end_char_idx=18595, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=1.1232140064239502),\n",
              " NodeWithScore(node=TextNode(id_='f2c37dec-1fde-4662-9da5-ed9784905050', embedding=None, metadata={'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ad6fee52-f00f-4224-97a5-d23758b3a213', node_type='4', metadata={'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}, hash='0c3c3f46cac874b495d944dfc4b920f6b68817dbbb1699ecc955d1fafb2bf87b'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4f0d7136-3bf3-4099-924a-1c78598434a9', node_type='1', metadata={'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}, hash='e363d7c644825cd463543c35ab2894aba7b68b1537eecd0d5424517ad6a24c3a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='8cb986ad-086d-4d30-9b91-06387dea6870', node_type='1', metadata={}, hash='7c37dfed7f0f4cce1a6c95f061b41bc05324a8c91b043bbd22afe1b9754cb48c')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"People might mention your software in footnotes, but no one would actually use it. And indeed, it would seem very feeble work. Only people with a sense of the history of the field would even realize that, in its time, it had been good.\\n\\nThere were some surplus Xerox Dandelions floating around the computer lab at one point. Anyone who wanted one to play around with could have one. I was briefly tempted, but they were so slow by present standards; what was the point? No one else wanted one either, so off they went. That was what happened to systems work.\\n\\nI wanted not just to build things, but to build things that would last.\\n\\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where he was in grad school. One day I went to visit the Carnegie Institute, where I'd spent a lot of time as a kid. While looking at a painting there I realized something that might seem obvious, but was a big surprise to me. There, right on the wall, was something you could make that would last. Paintings didn't become obsolete. Some of the best ones were hundreds of years old.\\n\\nAnd moreover this was something you could make a living doing. Not as easily as you could by writing software, of course, but I thought if you were really industrious and lived really cheaply, it had to be possible to make enough to survive. And as an artist you could be truly independent. You wouldn't have a boss, or even need to get research funding.\\n\\nI had always liked looking at paintings. Could I make them? I had no idea. I'd never imagined it was even possible. I knew intellectually that people made art — that it didn't just appear spontaneously — but it was as if the people who made it were a different species. They either lived long ago or were mysterious geniuses doing strange things in profiles in Life magazine. The idea of actually being able to make art, to put that verb before that noun, seemed almost miraculous.\\n\\nThat fall I started taking art classes at Harvard. Grad students could take classes in any department, and my advisor, Tom Cheatham, was very easy going. If he even knew about the strange classes I was taking, he never said anything.\", mimetype='text/plain', start_char_idx=8286, end_char_idx=10446, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=1.1505811214447021)]"
            ]
          },
          "execution_count": 98,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever = vector_index.as_retriever(similarity_top_k=2)\n",
        "retriever.retrieve(\"What did the author do growing up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06J6cZ-jRm64",
        "outputId": "ff511e24-6daf-470a-8747-554b2cdff5d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retrieverl = loaded_index.as_retriever(similarity_top_k=2)\n",
        "retrieverl.retrieve(\"What did the author do growing up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hs3Eq-kxib3l",
        "outputId": "fe8ffd2b-0efd-4826-ade3-c3e8ea3f5bdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(loaded_index.docstore.docs.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuwBKnXejVZG",
        "outputId": "f6c7fe7d-3a20-40ad-cd6f-fcb08116f9e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(llama_index.core.indices.vector_store.base.VectorStoreIndex,\n",
              " llama_index.core.indices.vector_store.base.VectorStoreIndex)"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(loaded_index), type(vector_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLZF0OovjmQf",
        "outputId": "b3276a0d-1ff8-40d2-bb48-f201c94b2d1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Response(response='Empty Response', source_nodes=[], metadata=None)"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qe = loaded_index.as_query_engine()\n",
        "qe.query(\"What did the author do growing up?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "POhYVEogkXBD",
        "outputId": "301024be-5630-4d5e-d171-a116278d77aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Response(response=\"The provided text mentions the author spent a lot of time at the Carnegie Institute as a child.  Beyond that, the text does not describe the author's childhood activities.\\n\", source_nodes=[NodeWithScore(node=TextNode(id_='3e94e0c4-7ba9-4a11-ad4f-80fb2d55cc09', embedding=None, metadata={'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ad6fee52-f00f-4224-97a5-d23758b3a213', node_type='4', metadata={'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}, hash='0c3c3f46cac874b495d944dfc4b920f6b68817dbbb1699ecc955d1fafb2bf87b'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c00aefd3-4d0a-45a8-8784-b7c6a99c739d', node_type='1', metadata={'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}, hash='928bba6f9657c9dfac691a9697530ff62ed4c976c6f8d5fc469ed86fc7e6e419'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='af572889-0175-42b9-a622-29574c176917', node_type='1', metadata={}, hash='c0d6eec84f92ccdad207f544d047fc5fd9d1d9fd57ee03d59760ab17e1404dd0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"In everyday life it would be distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and when you do there's a lot to see. You can still be noticing new things after days of trying to paint something people usually take for granted, just as you can after days of trying to write an essay about something people usually take for granted.\\n\\nThis is not the only way to paint. I'm not 100% sure it's even a good way to paint. But it seemed a good enough bet to be worth trying.\\n\\nOur teacher, professor Ulivi, was a nice guy. He could see I worked hard, and gave me a good grade, which he wrote down in a sort of passport each student had. But the Accademia wasn't teaching me anything except Italian, and my money was running out, so at the end of the first year I went back to the US.\\n\\nI wanted to go back to RISD, but I was now broke and RISD was very expensive, so I decided to get a job for a year and then return to RISD the next fall. I got one at a company called Interleaf, which made software for creating documents. You mean like Microsoft Word? Exactly. That was how I learned that low end software tends to eat high end software. But Interleaf still had a few years to live yet. [5]\\n\\nInterleaf had done something pretty bold. Inspired by Emacs, they'd added a scripting language, and even made the scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing I've had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was the thinnest icing on a giant C cake, and since I didn't know C and didn't want to learn it, I never understood most of the software. Plus I was terribly irresponsible. This was back when a programming job meant showing up every day during certain working hours. That seemed unnatural to me, and on this point the rest of the world is coming around to my way of thinking, but at the time it caused a lot of friction.\", mimetype='text/plain', start_char_idx=16574, end_char_idx=18595, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=1.1232140064239502), NodeWithScore(node=TextNode(id_='f2c37dec-1fde-4662-9da5-ed9784905050', embedding=None, metadata={'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='ad6fee52-f00f-4224-97a5-d23758b3a213', node_type='4', metadata={'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}, hash='0c3c3f46cac874b495d944dfc4b920f6b68817dbbb1699ecc955d1fafb2bf87b'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='4f0d7136-3bf3-4099-924a-1c78598434a9', node_type='1', metadata={'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}, hash='e363d7c644825cd463543c35ab2894aba7b68b1537eecd0d5424517ad6a24c3a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='8cb986ad-086d-4d30-9b91-06387dea6870', node_type='1', metadata={}, hash='7c37dfed7f0f4cce1a6c95f061b41bc05324a8c91b043bbd22afe1b9754cb48c')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"People might mention your software in footnotes, but no one would actually use it. And indeed, it would seem very feeble work. Only people with a sense of the history of the field would even realize that, in its time, it had been good.\\n\\nThere were some surplus Xerox Dandelions floating around the computer lab at one point. Anyone who wanted one to play around with could have one. I was briefly tempted, but they were so slow by present standards; what was the point? No one else wanted one either, so off they went. That was what happened to systems work.\\n\\nI wanted not just to build things, but to build things that would last.\\n\\nIn this dissatisfied state I went in 1988 to visit Rich Draves at CMU, where he was in grad school. One day I went to visit the Carnegie Institute, where I'd spent a lot of time as a kid. While looking at a painting there I realized something that might seem obvious, but was a big surprise to me. There, right on the wall, was something you could make that would last. Paintings didn't become obsolete. Some of the best ones were hundreds of years old.\\n\\nAnd moreover this was something you could make a living doing. Not as easily as you could by writing software, of course, but I thought if you were really industrious and lived really cheaply, it had to be possible to make enough to survive. And as an artist you could be truly independent. You wouldn't have a boss, or even need to get research funding.\\n\\nI had always liked looking at paintings. Could I make them? I had no idea. I'd never imagined it was even possible. I knew intellectually that people made art — that it didn't just appear spontaneously — but it was as if the people who made it were a different species. They either lived long ago or were mysterious geniuses doing strange things in profiles in Life magazine. The idea of actually being able to make art, to put that verb before that noun, seemed almost miraculous.\\n\\nThat fall I started taking art classes at Harvard. Grad students could take classes in any department, and my advisor, Tom Cheatham, was very easy going. If he even knew about the strange classes I was taking, he never said anything.\", mimetype='text/plain', start_char_idx=8286, end_char_idx=10446, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=1.1505811214447021)], metadata={'3e94e0c4-7ba9-4a11-ad4f-80fb2d55cc09': {'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}, 'f2c37dec-1fde-4662-9da5-ed9784905050': {'file_path': '/content/data/paul_graham_essay.txt', 'file_name': 'paul_graham_essay.txt', 'file_type': 'text/plain', 'file_size': 75042, 'creation_date': '2025-01-22', 'last_modified_date': '2025-01-22'}})"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qe = vector_index.as_query_engine()\n",
        "qe.query(\"What did the author do growing up?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlxGm1HckbTg"
      },
      "source": [
        "Loaded Index Retriever and Query Engine doesn't work but VectorIndex works.\n",
        "\n",
        "This is a problem. Why?\n",
        "\n",
        "Assume you're deploy this system in Containerized environment, We can't create an index during scale up, restart etc. This is waste and increases downtime.\n",
        "\n",
        "Find a fix or discuss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXT2w_ny3MuP"
      },
      "source": [
        "#### New index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EylnJXJCkt1r",
        "outputId": "81e6d9de-862e-478e-a4c3-e5c71a0b9aa3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29/29 [01:36<00:00,  3.31s/it]\n"
          ]
        }
      ],
      "source": [
        "# Evaluate for a single k\n",
        "from llama_index.core.evaluation import RetrieverEvaluator\n",
        "import asyncio\n",
        "retriever = vector_index.as_retriever(top_k=2)\n",
        "evaluator = RetrieverEvaluator.from_metric_names(\n",
        "    retriever=retriever,\n",
        "    metric_names=[\"hit_rate\", \"mrr\"],\n",
        ")\n",
        "loop = asyncio.get_event_loop()\n",
        "# Provides only async evaluation\n",
        "results = await evaluator.aevaluate_dataset(dataset=qa_loaded_ds, show_progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7r1dO3efoT4h",
        "outputId": "414bea83-95d3-45aa-97dc-ecbefb49d8b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': \"What limitations of the IBM 1401 computer hampered the author's early programming efforts, and what specific example illustrates this limitation?\",\n",
              " 'expected_ids': ['e4412451-913f-4443-8984-6ec1976b3779'],\n",
              " 'expected_texts': None,\n",
              " 'retrieved_ids': ['abe47102-de9f-464f-9e05-fb5bb4ccbf5f',\n",
              "  '3e94e0c4-7ba9-4a11-ad4f-80fb2d55cc09'],\n",
              " 'retrieved_texts': [\"Then in November, right in the middle of a painting, I ran out of steam. Up till that point I'd always been curious to see how the painting I was working on would turn out, but suddenly finishing this one seemed like a chore. So I stopped working on it and cleaned my brushes and haven't painted since. So far anyway.\\n\\nI realize that sounds rather wimpy. But attention is a zero sum game. If you can choose what to work on, and you choose a project that's not the best one (or at least a good one) for you, then it's getting in the way of another project that is. And at 50 there was some opportunity cost to screwing around.\\n\\nI started writing essays again, and wrote a bunch of new ones over the next few months. I even wrote a couple that weren't about startups. Then in March 2015 I started working on Lisp again.\\n\\nThe distinctive thing about Lisp is that its core is a language defined by writing an interpreter in itself. It wasn't originally intended as a programming language in the ordinary sense. It was meant to be a formal model of computation, an alternative to the Turing machine. If you want to write an interpreter for a language in itself, what's the minimum set of predefined operators you need? The Lisp that John McCarthy invented, or more accurately discovered, is an answer to that question. [19]\\n\\nMcCarthy didn't realize this Lisp could even be used to program computers till his grad student Steve Russell suggested it. Russell translated McCarthy's interpreter into IBM 704 machine language, and from that point Lisp started also to be a programming language in the ordinary sense. But its origins as a model of computation gave it a power and elegance that other languages couldn't match. It was this that attracted me in college, though I didn't understand why at the time.\\n\\nMcCarthy's 1960 Lisp did nothing more than interpret Lisp expressions. It was missing a lot of things you'd want in a programming language. So these had to be added, and when they were, they weren't defined using McCarthy's original axiomatic approach. That wouldn't have been feasible at the time. McCarthy tested his interpreter by hand-simulating the execution of programs.\",\n",
              "  \"In everyday life it would be distracting to notice every leaf on every bush. But when you have to paint something, you have to look more closely, and when you do there's a lot to see. You can still be noticing new things after days of trying to paint something people usually take for granted, just as you can after days of trying to write an essay about something people usually take for granted.\\n\\nThis is not the only way to paint. I'm not 100% sure it's even a good way to paint. But it seemed a good enough bet to be worth trying.\\n\\nOur teacher, professor Ulivi, was a nice guy. He could see I worked hard, and gave me a good grade, which he wrote down in a sort of passport each student had. But the Accademia wasn't teaching me anything except Italian, and my money was running out, so at the end of the first year I went back to the US.\\n\\nI wanted to go back to RISD, but I was now broke and RISD was very expensive, so I decided to get a job for a year and then return to RISD the next fall. I got one at a company called Interleaf, which made software for creating documents. You mean like Microsoft Word? Exactly. That was how I learned that low end software tends to eat high end software. But Interleaf still had a few years to live yet. [5]\\n\\nInterleaf had done something pretty bold. Inspired by Emacs, they'd added a scripting language, and even made the scripting language a dialect of Lisp. Now they wanted a Lisp hacker to write things in it. This was the closest thing I've had to a normal job, and I hereby apologize to my boss and coworkers, because I was a bad employee. Their Lisp was the thinnest icing on a giant C cake, and since I didn't know C and didn't want to learn it, I never understood most of the software. Plus I was terribly irresponsible. This was back when a programming job meant showing up every day during certain working hours. That seemed unnatural to me, and on this point the rest of the world is coming around to my way of thinking, but at the time it caused a lot of friction.\"],\n",
              " 'mode': <RetrievalEvalMode.TEXT: 'text'>,\n",
              " 'metric_dict': {'hit_rate': RetrievalMetricResult(score=0.0, metadata={}),\n",
              "  'mrr': RetrievalMetricResult(score=0.0, metadata={})}}"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vars(results[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syCDOWWopDl8"
      },
      "source": [
        "We can pass arg `mode` to evaluate it for text as well. It's an enum and accepts two values. `text` and `image.`\n",
        "\n",
        "So Evaluator runs retrieval evaluation against each query and returns `llama_index.core.evaluation.retrieval.base.RetrievalEvalResult` object. What we need is `metric_dict` which has the metrics we need. Let's put all the results into a dataframe and group them by K and calculate the mean of metrics.\n",
        "\n",
        "Time Taken for Single K, 29 data points is 1 minute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "3G_pBg2grGZK",
        "outputId": "71c59054-2572-4cbf-b838-3275f9d80e75"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>llama_index.core.evaluation.retrieval.base.RetrievalEvalResult</b><br/>def __init__(self, /, **data: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/llama_index/core/evaluation/retrieval/base.py</a>Retrieval eval result.\n",
              "\n",
              "NOTE: this abstraction might change in the future.\n",
              "\n",
              "Attributes:\n",
              "    query (str): Query string\n",
              "    expected_ids (List[str]): Expected ids\n",
              "    retrieved_ids (List[str]): Retrieved ids\n",
              "    metric_dict (Dict[str, BaseRetrievalMetric]):             Metric dictionary for the evaluation</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 36);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "llama_index.core.evaluation.retrieval.base.RetrievalEvalResult"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(results[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbT2esVclw4F",
        "outputId": "46c370f2-a346-41e1-e4a3-9cc4a9cc1176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating for k=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29/29 [01:30<00:00,  3.12s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating for k=2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29/29 [01:25<00:00,  2.96s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating for k=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29/29 [01:28<00:00,  3.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating for k=4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29/29 [01:27<00:00,  3.02s/it]\n"
          ]
        }
      ],
      "source": [
        "# Evaluating for multiple K values\n",
        "from llama_index.core.evaluation import RetrieverEvaluator\n",
        "K_VALUES = range(1,5,1)\n",
        "all_evals = []\n",
        "for k in K_VALUES:\n",
        "  print(f\"Evaluating for k={k}\")\n",
        "  retriever = vector_index.as_retriever(top_k=k)\n",
        "  evaluator = RetrieverEvaluator.from_metric_names(\n",
        "      retriever=retriever,\n",
        "      metric_names=[\"hit_rate\", \"mrr\"],\n",
        "  )\n",
        "  eval_results = await evaluator.aevaluate_dataset(qa_loaded_ds, show_progress=True)\n",
        "  all_evals.append(eval_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9fdJs5cBKdv",
        "outputId": "6d84ccf5-135c-40ed-d107-61266a57456f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(all_evals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7IyW8FA3d6T"
      },
      "outputs": [],
      "source": [
        "metrics = []\n",
        "for k, k_result in enumerate(all_evals):\n",
        "  for query_result in k_result:\n",
        "      metrics.append({\n",
        "          \"k\": k,\n",
        "          \"hit_rate\": query_result.metric_dict[\"hit_rate\"].score,\n",
        "          \"mrr\": query_result.metric_dict[\"mrr\"].score,\n",
        "          # For visibility\n",
        "          \"query\": query_result.query,\n",
        "          \"expected_ids\": query_result.expected_ids,\n",
        "          \"predicted_ids\": query_result.retrieved_ids,\n",
        "      })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF0Yb1SCBEfs",
        "outputId": "386617b2-12b8-4d74-9b57-a5fe6659ea95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "116"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbZtACQpk-bi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "metrics_df = pd.DataFrame(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "ZNbUhCHqAkj9",
        "outputId": "647204db-75f3-4fec-c667-7e35ba9b68b0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"metrics_df\",\n  \"rows\": 116,\n  \"fields\": [\n    {\n      \"column\": \"k\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hit_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mrr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"query\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"Explain the significance of the Y Combinator (YC) logo's color and design, referencing the context provided.  Why was this choice made, and what does it reveal about the organization's early strategy and identity?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"expected_ids\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted_ids\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "metrics_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-824657fa-0ec2-4729-8bc7-b008771ff2a3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>k</th>\n",
              "      <th>hit_rate</th>\n",
              "      <th>mrr</th>\n",
              "      <th>query</th>\n",
              "      <th>expected_ids</th>\n",
              "      <th>predicted_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>What limitations of the IBM 1401 computer hamp...</td>\n",
              "      <td>[e4412451-913f-4443-8984-6ec1976b3779]</td>\n",
              "      <td>[ce40434e-a6e6-4a61-85c9-dad214d4f4ae, 7294929...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>What factors influenced the author's decision ...</td>\n",
              "      <td>[c46ea1fb-acbb-4bae-81db-651faaabe9c5]</td>\n",
              "      <td>[095132a0-ee44-4df5-bf80-9a716291325d, 041dbc3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>What factors contributed to the author's decis...</td>\n",
              "      <td>[d717506d-a279-42fd-be75-95b22fd36127]</td>\n",
              "      <td>[72949294-13cc-4ba4-95c3-170176cf581e, ce40434...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Describe the author's disillusionment with AI ...</td>\n",
              "      <td>[4f0d7136-3bf3-4099-924a-1c78598434a9]</td>\n",
              "      <td>[095132a0-ee44-4df5-bf80-9a716291325d, 69458f9...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>What realization did the author have while vie...</td>\n",
              "      <td>[f2c37dec-1fde-4662-9da5-ed9784905050]</td>\n",
              "      <td>[041dbc31-6ec2-41ba-aa81-f622168974c5, a497dc7...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-824657fa-0ec2-4729-8bc7-b008771ff2a3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-824657fa-0ec2-4729-8bc7-b008771ff2a3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-824657fa-0ec2-4729-8bc7-b008771ff2a3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9074538e-58d0-4b3c-9275-4f63774e121d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9074538e-58d0-4b3c-9275-4f63774e121d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9074538e-58d0-4b3c-9275-4f63774e121d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   k  hit_rate  mrr                                              query  \\\n",
              "0  0       0.0  0.0  What limitations of the IBM 1401 computer hamp...   \n",
              "1  0       0.0  0.0  What factors influenced the author's decision ...   \n",
              "2  0       0.0  0.0  What factors contributed to the author's decis...   \n",
              "3  0       0.0  0.0  Describe the author's disillusionment with AI ...   \n",
              "4  0       0.0  0.0  What realization did the author have while vie...   \n",
              "\n",
              "                             expected_ids  \\\n",
              "0  [e4412451-913f-4443-8984-6ec1976b3779]   \n",
              "1  [c46ea1fb-acbb-4bae-81db-651faaabe9c5]   \n",
              "2  [d717506d-a279-42fd-be75-95b22fd36127]   \n",
              "3  [4f0d7136-3bf3-4099-924a-1c78598434a9]   \n",
              "4  [f2c37dec-1fde-4662-9da5-ed9784905050]   \n",
              "\n",
              "                                       predicted_ids  \n",
              "0  [ce40434e-a6e6-4a61-85c9-dad214d4f4ae, 7294929...  \n",
              "1  [095132a0-ee44-4df5-bf80-9a716291325d, 041dbc3...  \n",
              "2  [72949294-13cc-4ba4-95c3-170176cf581e, ce40434...  \n",
              "3  [095132a0-ee44-4df5-bf80-9a716291325d, 69458f9...  \n",
              "4  [041dbc31-6ec2-41ba-aa81-f622168974c5, a497dc7...  "
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDgqDfTzBBPh",
        "outputId": "14f20067-b2f6-4f2d-cb8d-bd0d085dc642"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics_df.k.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQR-I-aXA4ou",
        "outputId": "2ffa2036-426f-4462-dbb5-b7832a0de38d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(k\n",
              " 0    0.0\n",
              " 1    0.0\n",
              " 2    0.0\n",
              " 3    0.0\n",
              " Name: hit_rate, dtype: float64,\n",
              " k\n",
              " 0    0.0\n",
              " 1    0.0\n",
              " 2    0.0\n",
              " 3    0.0\n",
              " Name: mrr, dtype: float64)"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics_df.groupby(\"k\")[\"hit_rate\"].mean(), metrics_df.groupby(\"k\")[\"mrr\"].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4DQzdUIqw3Z"
      },
      "outputs": [],
      "source": [
        "# from llama_index.core.evaluation.retrieval.metrics import HitRate, MRR\n",
        "# To Do, low level RetrieverEvaluation directly with metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8qL7z5L3igc"
      },
      "source": [
        "Retrieval metrics is zero. No retrieved docs match the questions. This might be due to one of the below reasons:\n",
        "1. Syntheic Data Quality(Depends on LLM, Because it's generated with a prompt and the text).\n",
        "2. Chunk(Chunk might be big for generation, number of questions(1) might be small). Let's play with these and see if we can generate a better dataset.\n",
        "3. Embeddings is not capturing the similarity. (Think of it questions is generated by Gemini, but embeddings model is qwen.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ-zi-hS3jRU"
      },
      "source": [
        "### DatasetV1\n",
        "\n",
        "We'll create a new dataset with different chunking strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVUVp7JhvW73"
      },
      "source": [
        "#### Changes\n",
        "\n",
        "1. Chunk Size reduced\n",
        "2. Faiss Index HNSW --> L2(Euclidean)\n",
        "3. gte-qwen-embedding --> text-embedding-004(Gemini)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lklPA6DhuOgn"
      },
      "outputs": [],
      "source": [
        "# Create nodes/chunks\n",
        "sentence_splitter_smal = SentenceSplitter(chunk_size=256, chunk_overlap=32)\n",
        "nodes = sentence_splitter_smal.get_nodes_from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQDOSz4juQFi",
        "outputId": "8ff1633d-69d5-4212-b4ac-4c78563e96ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "execution_count": 166,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed22ihVO_MNq"
      },
      "outputs": [],
      "source": [
        "pickle.dump(nodes, open(os.path.join(EXP_DIR, \"pg-nodes-256c-32co-gemini.pkl\"), \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "-2j0u0iZeP24",
        "outputId": "12b9ff0c-d183-40bd-9462-8ad01b9d14a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test\n",
        "test_embedding = embed_model_gemini.get_text_embedding(\"Hello\")\n",
        "len(test_embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swKTcQkafuwK"
      },
      "outputs": [],
      "source": [
        "embed_dim_v1 = len(test_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHHcZd8hyRQr"
      },
      "source": [
        "Local embeddings took 10minutes for the same document with higher chunk size but Maintained Embeddings accessed via API is real quick. The Decoupling is powerful in this case.\n",
        "\n",
        "But with cost in mind it's best to have a load and save mechanism to avoid generating embeddings for same dataset and embedding model.\n",
        "\n",
        "Let's try saving with L2, because HNSW didn't workout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XVMSdTzj_RO"
      },
      "source": [
        "***Peculiar problem with index creation***\n",
        "\n",
        "Indexes get's added to `storage_context.index_store` as a list. And the specific index_id, nodes_dict is used when using the index as retriever. In llama-index even after instantiating a new `StorageContext` the id's `StorageContext.index_store.index_structs()` a new index is created and the ids in `StorageContext.index_store.index_structs()[idx].node_dict` ids gets incremented. Problem? While retrieve with index, it expects keys from 0. But due to the incrementic nature, this doesn't work.\n",
        "\n",
        "```\n",
        "index_v1 = VectorStoreIndex(\n",
        "    nodes=nodes,\n",
        "    storage_context=storage_context,\n",
        "    embed_model=embed_model_gemini,\n",
        "    show_progress=True,\n",
        ")\n",
        "```\n",
        "Extends the indexes list and increments the id's in nodes_dict.\n",
        "\n",
        "Let's recreate a new index. To do that a new storage context is required. There's also function to delete_indexes with `delete_index_struct()`. We can use this to clean up the existing indices as well. The `node_dict` ids increment continues even after this.\n",
        "\n",
        "* `storage_context.index_store.index_structs()` To get the list of indices.\n",
        "* Each item has below attributes`dict_keys(['index_id', 'summary', 'nodes_dict', 'doc_id_dict', 'embeddings_dict'])\n",
        "`.\n",
        "\n",
        "* Use the `index_id` to access a specific index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BW5DV3SyYT3"
      },
      "outputs": [],
      "source": [
        "# Create index\n",
        "faiss_index = faiss.IndexFlatL2(embed_dim_v1)\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=vector_store,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctEReAkHy3Zh",
        "outputId": "aed43124-b02f-4ba1-8d5d-2b222cdb562c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check for any existing indices\n",
        "storage_context.index_store.index_structs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "12840615d2e445c29032fee20d9a404f",
            "23570a4a0ace43368344754cf68e827b",
            "52c68508da7242fb838f1f12be366cb6",
            "253880f2060546da8773033e9451b124",
            "9ea9907194a54cef9273bdb1fd3ffd9d",
            "af15b3eb95d1476084bdda3c5c37f8db",
            "2198d5ab1ba94d9fbd72c2995e8b8a89",
            "e1d69ff3d7574328b4a2b59303cda3d5",
            "f3815231cff044f595445fc03dd1030a",
            "e75081e2edcd42e7b504501f9de2f340",
            "daed53877c8845b18f06ac0438cb67e0"
          ]
        },
        "id": "6PoN9gnwy7ab",
        "outputId": "e1e928e8-df0a-4d4a-b63b-e265fc817a27"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12840615d2e445c29032fee20d9a404f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating embeddings:   0%|          | 0/83 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create new index\n",
        "index_faiss_l2 = VectorStoreIndex(\n",
        "    nodes=nodes,\n",
        "    storage_context=storage_context,\n",
        "    embed_model=embed_model_gemini,\n",
        "    show_progress=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u35c6vFS0F2u"
      },
      "outputs": [],
      "source": [
        "# Assert all nodes are present in nodes dict\n",
        "assert storage_context.index_store.index_structs()[0].nodes_dict.__len__() == len(nodes)\n",
        "# Assert indexes are correct\n",
        "assert int(list(storage_context.index_store.index_structs()[0].nodes_dict.keys())[-1]) == len(nodes)-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF0ibzQt17KK"
      },
      "outputs": [],
      "source": [
        "# Store index\n",
        "# Index dir\n",
        "EXP_INDEX_DIR = os.path.join(EXP_DIR, \"index\")\n",
        "os.makedirs(os.path.join(EXP_DIR, \"index\"), exist_ok=True)\n",
        "index_faiss_l2.storage_context.persist(EXP_INDEX_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX3gr9Q90JUH"
      },
      "outputs": [],
      "source": [
        "# load index from disk\n",
        "EXP_INDEX_DIR = os.path.join(EXP_DIR, \"index\")\n",
        "Settings.llm = llm_gemini\n",
        "Settings.embed_model = embed_model_gemini\n",
        "vector_store = FaissVectorStore.from_persist_dir(EXP_INDEX_DIR)\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=vector_store, persist_dir=EXP_INDEX_DIR,\n",
        ")\n",
        "index = load_index_from_storage(storage_context=storage_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLd-bKvB3GNA"
      },
      "outputs": [],
      "source": [
        "# Test loaded index\n",
        "retriever = index.as_retriever(similarity_top_k=2)\n",
        "retrieved_nodes = retriever.retrieve(\"What did the author do growing up?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgB2KXQq3d_p"
      },
      "source": [
        "#### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVRW2b3dr-E1"
      },
      "outputs": [],
      "source": [
        "slice_idx = list(range(0, len(nodes), 10))\n",
        "slice_idx.append(len(nodes)) if len(nodes) not in slice_idx else None\n",
        "# Create data slices\n",
        "slices = []\n",
        "for i in range(len(slice_idx)-1):\n",
        "  slices.append(nodes[slice_idx[i]:slice_idx[i+1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx_zbiizy0-x",
        "outputId": "9063a266-40f8-47d0-e384-4f643dee74a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 181,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(slice_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZskjXpPy5Bw",
        "outputId": "84f93b4d-7d30-4c30-9f0b-1520d960c8bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "execution_count": 182,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuT7Y1IfunFb",
        "outputId": "13db947b-bac5-4753-96da-c5eaa7d79406"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 183,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "slices[0][0].text == slices[1][0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pTzjYOSVsbj_",
        "outputId": "2be3b5e1-18a2-42e0-bb58-5b9798330365"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|█         | 1/10 [00:02<00:20,  2.23s/it]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 2/10 [00:04<00:15,  1.98s/it]\u001b[A\u001b[A\n",
            "\n",
            " 30%|███       | 3/10 [00:06<00:13,  1.98s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 4/10 [00:07<00:11,  1.96s/it]\u001b[A\u001b[A\n",
            "\n",
            " 50%|█████     | 5/10 [00:09<00:09,  1.95s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 6/10 [00:12<00:08,  2.03s/it]\u001b[A\u001b[A\n",
            "\n",
            " 70%|███████   | 7/10 [00:14<00:06,  2.08s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 8/10 [00:16<00:04,  2.10s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 9/10 [00:18<00:01,  2.00s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 10/10 [00:19<00:00,  2.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|█         | 1/10 [00:01<00:15,  1.76s/it]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 2/10 [00:03<00:14,  1.84s/it]\u001b[A\u001b[A\n",
            "\n",
            " 30%|███       | 3/10 [00:05<00:13,  1.92s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 4/10 [00:07<00:11,  1.93s/it]\u001b[A\u001b[A\n",
            "\n",
            " 50%|█████     | 5/10 [00:09<00:09,  1.93s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 6/10 [00:11<00:07,  1.90s/it]\u001b[A\u001b[A\n",
            "\n",
            " 70%|███████   | 7/10 [00:13<00:05,  1.83s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 8/10 [00:15<00:03,  1.89s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 9/10 [00:16<00:01,  1.82s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 10/10 [00:18<00:00,  1.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|█         | 1/10 [00:01<00:17,  1.90s/it]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 2/10 [00:03<00:15,  1.95s/it]\u001b[A\u001b[A\n",
            "\n",
            " 30%|███       | 3/10 [00:06<00:14,  2.03s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 4/10 [00:08<00:12,  2.02s/it]\u001b[A\u001b[A\n",
            "\n",
            " 50%|█████     | 5/10 [00:09<00:09,  1.97s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 6/10 [00:11<00:07,  1.93s/it]\u001b[A\u001b[A\n",
            "\n",
            " 70%|███████   | 7/10 [00:13<00:05,  1.91s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 8/10 [00:15<00:03,  1.91s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 9/10 [00:17<00:02,  2.00s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 10/10 [00:19<00:00,  1.97s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|█         | 1/10 [00:01<00:16,  1.88s/it]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 2/10 [00:03<00:15,  1.97s/it]\u001b[A\u001b[A\n",
            "\n",
            " 30%|███       | 3/10 [00:05<00:14,  2.02s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 4/10 [00:08<00:12,  2.04s/it]\u001b[A\u001b[A\n",
            "\n",
            " 50%|█████     | 5/10 [00:10<00:10,  2.06s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 6/10 [00:12<00:08,  2.25s/it]\u001b[A\u001b[A\n",
            "\n",
            " 70%|███████   | 7/10 [00:15<00:06,  2.25s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 8/10 [00:17<00:04,  2.30s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 9/10 [00:19<00:02,  2.32s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 10/10 [00:21<00:00,  2.19s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|█         | 1/10 [00:02<00:20,  2.29s/it]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 2/10 [00:04<00:18,  2.35s/it]\u001b[A\u001b[A\n",
            "\n",
            " 30%|███       | 3/10 [00:06<00:14,  2.13s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 4/10 [00:08<00:13,  2.21s/it]\u001b[A\u001b[A\n",
            "\n",
            " 50%|█████     | 5/10 [00:10<00:10,  2.08s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 6/10 [00:12<00:08,  2.04s/it]\u001b[A\u001b[A\n",
            "\n",
            " 70%|███████   | 7/10 [00:14<00:05,  1.97s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 8/10 [00:16<00:03,  2.00s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 9/10 [00:18<00:02,  2.01s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 10/10 [00:20<00:00,  2.08s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|█         | 1/10 [00:01<00:17,  1.93s/it]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 2/10 [00:04<00:16,  2.02s/it]\u001b[A\u001b[A\n",
            "\n",
            " 30%|███       | 3/10 [00:05<00:13,  1.95s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 4/10 [00:07<00:11,  1.94s/it]\u001b[A\u001b[A\n",
            "\n",
            " 50%|█████     | 5/10 [00:10<00:10,  2.03s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 6/10 [00:12<00:08,  2.07s/it]\u001b[A\u001b[A\n",
            "\n",
            " 70%|███████   | 7/10 [00:14<00:06,  2.14s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 8/10 [00:16<00:04,  2.23s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 9/10 [00:18<00:02,  2.19s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 10/10 [00:20<00:00,  2.09s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|█         | 1/10 [00:01<00:15,  1.74s/it]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 2/10 [00:03<00:15,  1.89s/it]\u001b[A\u001b[A\n",
            "\n",
            " 30%|███       | 3/10 [00:05<00:13,  1.91s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 4/10 [00:07<00:12,  2.00s/it]\u001b[A\u001b[A\n",
            "\n",
            " 50%|█████     | 5/10 [00:10<00:10,  2.09s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 6/10 [00:11<00:07,  1.99s/it]\u001b[A\u001b[A\n",
            "\n",
            " 70%|███████   | 7/10 [00:13<00:05,  1.90s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 8/10 [00:15<00:03,  1.84s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 9/10 [00:17<00:01,  1.89s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 10/10 [00:19<00:00,  1.94s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 10%|█         | 1/10 [00:02<00:19,  2.12s/it]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 2/10 [00:04<00:16,  2.03s/it]\u001b[A\u001b[A\n",
            "\n",
            " 30%|███       | 3/10 [00:05<00:13,  1.93s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 4/10 [00:07<00:11,  1.99s/it]\u001b[A\u001b[A\n",
            "\n",
            " 50%|█████     | 5/10 [00:10<00:10,  2.05s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 6/10 [00:12<00:08,  2.01s/it]\u001b[A\u001b[A\n",
            "\n",
            " 70%|███████   | 7/10 [00:13<00:05,  1.96s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 8/10 [00:16<00:04,  2.20s/it]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 9/10 [00:18<00:02,  2.24s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 10/10 [00:20<00:00,  2.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 33%|███▎      | 1/3 [00:01<00:03,  1.88s/it]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 2/3 [00:03<00:01,  1.96s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 3/3 [00:06<00:00,  2.04s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset saved.\n"
          ]
        }
      ],
      "source": [
        "# Generate QA dataset\n",
        "import time\n",
        "qa_datasets = []\n",
        "for idx, data_slice in enumerate(slices):\n",
        "  qa_dataset_gen = generate_qa_embedding_pairs(\n",
        "      nodes=data_slice,\n",
        "      llm=llm_gemini,\n",
        "      num_questions_per_chunk=2,\n",
        "      verbose=True,\n",
        "      output_path=f\"qa_ft_ds_{idx}.json\", # If iterating specify this. Otherwise on subsequent iterations default value qa_finetune_dataset.json will be loaded and generation will be skipped.\n",
        "      )\n",
        "  qa_datasets.append(qa_dataset_gen)\n",
        "  time.sleep(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "HyGlw8yMMBFl",
        "outputId": "18761f87-073c-462f-8300-44a24a544767"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:17<00:00,  1.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset saved.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Slice 1 was not complete due to rate limits.\n",
        "qa_dataset_gen = generate_qa_embedding_pairs(\n",
        "      nodes=slices[1],\n",
        "      llm=llm_gemini,\n",
        "      num_questions_per_chunk=2,\n",
        "      verbose=True,\n",
        "      output_path=f\"qa_ft_ds_temp.json\", # If iterating specify this. Otherwise on subsequent iterations default value qa_finetune_dataset.json will be loaded and generation will be skipped.\n",
        "      )\n",
        "qa_datasets[1] = qa_dataset_gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRL5nn045NQP"
      },
      "outputs": [],
      "source": [
        "# List to store queries, corpus, relevant_docs\n",
        "qa_dataset_all_dict = {\n",
        "    \"queries\": {},\n",
        "    \"corpus\": {},\n",
        "    \"relevant_docs\": {},\n",
        "}\n",
        "\n",
        "for idx in range(len(qa_datasets)):\n",
        "  qa_dataset_all_dict[\"queries\"].update(qa_datasets[idx].queries)\n",
        "  qa_dataset_all_dict[\"corpus\"].update(qa_datasets[idx].corpus)\n",
        "  qa_dataset_all_dict[\"relevant_docs\"].update(qa_datasets[idx].relevant_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqKQxcrmJMUV"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "qa_dataset_path = os.path.join(EXP_DIR, \"qa_dataset_all.json\")\n",
        "with open(qa_dataset_path, \"w\") as f:\n",
        "  json.dump(qa_dataset_all_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erpw_o955v-y"
      },
      "outputs": [],
      "source": [
        "qa_dataset_path = os.path.join(EXP_DIR, \"qa_dataset_all.json\")\n",
        "qa_dataset_v1 = EmbeddingQAFinetuneDataset.from_json(qa_dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkGpEFgv_JrQ",
        "outputId": "c0d9fb54-17e3-4c00-e8f0-6c9177348350"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "166"
            ]
          },
          "execution_count": 189,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(qa_dataset_v1.queries) # This shoud be 83 * 2(questions per chunk) - 166"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "pGY_LWsGLyvO",
        "outputId": "613bb3d5-7071-4f92-e80c-340aaae4883d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines — CPU, disk drives, printer, card reader — sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran.'"
            ]
          },
          "execution_count": 221,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa_dataset_v1.corpus[nodes[0].id_]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMXGzXtYNF4M"
      },
      "source": [
        "#### Evaluate Retriever\n",
        "\n",
        "Let's import Hit Rate and MRR to have a fine grained metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es3HJLjSOGER"
      },
      "outputs": [],
      "source": [
        "# check loaded index for nodes\n",
        "assert storage_context.index_store.index_structs()[0].nodes_dict.__len__() == len(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXGutrs0NV7M"
      },
      "outputs": [],
      "source": [
        "# Intialize metrics\n",
        "hit_rate = HitRate(use_granular_hit_rate=True)\n",
        "mrr = MRR(use_granular_mrr=True)\n",
        "# Initialize retriever evaluator\n",
        "retriever = index.as_retriever(similarity_top_k=2)\n",
        "# Initialize evaluator\n",
        "evaluator = RetrieverEvaluator(\n",
        "  metrics = [hit_rate, mrr],\n",
        "  retriever=retriever,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "_nCxBAjYQBPa",
        "outputId": "319aab07-7068-4e20-ffa1-ef0f58a09b1b"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "object BatchEmbedContentsResponse can't be used in 'await' expression",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-137-acee3c6f1fcc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Provides only async evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aget_retrieved_ids_and_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_dataset_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'01209e51-9722-45a7-ab78-b77cb410e44f'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/evaluation/retrieval/evaluator.py\u001b[0m in \u001b[0;36m_aget_retrieved_ids_and_texts\u001b[0;34m(self, query, mode)\u001b[0m\n\u001b[1;32m     36\u001b[0m     ) -> Tuple[List[str], List[str]]:\n\u001b[1;32m     37\u001b[0m         \u001b[0;34m\"\"\"Get retrieved ids and texts, potentially applying a post-processor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mretrieved_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_postprocessors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/instrumentation/dispatcher.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m             )\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpanDropEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/base/base_retriever.py\u001b[0m in \u001b[0;36maretrieve\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m    274\u001b[0m                 \u001b[0mpayload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mEventPayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUERY_STR\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquery_bundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_str\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             ) as retrieve_event:\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_bundle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_bundle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 nodes = await self._ahandle_recursive_retrieval(\n\u001b[1;32m    278\u001b[0m                     \u001b[0mquery_bundle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_bundle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/instrumentation/dispatcher.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m             )\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpanDropEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/indices/vector_store/retrievers/retriever.py\u001b[0m in \u001b[0;36m_aretrieve\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mquery_bundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_bundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_strs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0membed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embed_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 embedding = await embed_model.aget_agg_embedding_from_queries(\n\u001b[0m\u001b[1;32m    112\u001b[0m                     \u001b[0mquery_bundle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_strs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/base/embeddings/base.py\u001b[0m in \u001b[0;36maget_agg_embedding_from_queries\u001b[0;34m(self, queries, agg_fn)\u001b[0m\n\u001b[1;32m    195\u001b[0m     ) -> Embedding:\n\u001b[1;32m    196\u001b[0m         \u001b[0;34m\"\"\"Async get aggregated embedding from multiple queries.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mquery_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maget_query_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0magg_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magg_fn\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmean_agg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0magg_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/base/embeddings/base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    195\u001b[0m     ) -> Embedding:\n\u001b[1;32m    196\u001b[0m         \u001b[0;34m\"\"\"Async get aggregated embedding from multiple queries.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mquery_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maget_query_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0magg_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magg_fn\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmean_agg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0magg_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/instrumentation/dispatcher.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m             )\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpanDropEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/base/embeddings/base.py\u001b[0m in \u001b[0;36maget_query_embedding\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mCBEventType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEMBEDDING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mEventPayload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSERIALIZED\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         ) as event:\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mquery_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aget_query_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             event.on_end(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/core/instrumentation/dispatcher.py\u001b[0m in \u001b[0;36masync_wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m             )\n\u001b[1;32m    366\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSpanDropEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/embeddings/gemini/base.py\u001b[0m in \u001b[0;36m_aget_query_embedding\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_aget_query_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;34m\"\"\"The asynchronous version of _get_query_embedding.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aget_text_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_aget_text_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_index/embeddings/gemini/base.py\u001b[0m in \u001b[0;36m_aget_text_embeddings\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_aget_text_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;34m\"\"\"Asynchronously get text embeddings.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         response = await self._model.embed_content_async(\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/generativeai/embedding.py\u001b[0m in \u001b[0;36membed_content_async\u001b[0;34m(model, content, task_type, title, output_dimensionality, client, request_options)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_batched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDING_MAX_BATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0membedding_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprotos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchEmbedContentsRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             embedding_response = await client.batch_embed_contents(\n\u001b[0m\u001b[1;32m    292\u001b[0m                 \u001b[0membedding_request\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/async_client.py\u001b[0m in \u001b[0;36mbatch_embed_contents\u001b[0;34m(self, request, model, requests, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m         response = await rpc(\n\u001b[0m\u001b[1;32m   1010\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object BatchEmbedContentsResponse can't be used in 'await' expression"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "loop = asyncio.get_event_loop()\n",
        "# Provides only async evaluation\n",
        "results = await evaluator._aget_retrieved_ids_and_texts(qa_dataset_v1.queries['01209e51-9722-45a7-ab78-b77cb410e44f'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzuy8izTVIEn"
      },
      "source": [
        "Batch Processing with Gemini Limit is 150. Let's customize `EmbeddingQAFinetuneDataset` to get slices of dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV2PAv1tWZ6O",
        "outputId": "fb49dc39-05a3-4276-bcc9-23b98e9d6cf6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'acb0d43f-8bc1-4f17-9567-4e7f779c0d91': \"Describe the author's early writing experiences, highlighting the type of writing they focused on and their self-assessment of their work.\",\n",
              " '44e539ba-a117-43b7-9c26-14093eeac249': 'What computer system did the author and Rich Draves use for their early programming endeavors, and what programming language did they employ?  Mention the location of this system within the school.',\n",
              " '4c0a0548-e20e-45ab-b1e5-56011accd014': \"Describe the input/output methods used in the early Fortran programming environment described in the passage, highlighting the limitations this imposed on the author's programming activities.\",\n",
              " '96e141f5-ad85-44ae-98c2-c763e3e0c85b': \"Explain the significance of the author's experience with a non-terminating program in the context of a machine without time-sharing.  What were the technical and social consequences?\",\n",
              " 'cf71a2e5-e05c-4a42-92a7-dacccbe98057': 'Describe the significant difference in user experience between using a mainframe computer (as implied in the first paragraph) and a microcomputer like the Heathkit or TRS-80, focusing on the immediacy of feedback and the interaction style.'}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict(list(qa_dataset_v1.queries.items())[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLZ7lovxV-6D"
      },
      "outputs": [],
      "source": [
        "# Function written to slice the dataset\n",
        "# This is written with 150 request limit for Gemini API BatchedEmbedContent\n",
        "from typing import List\n",
        "class EmbeddingQAFinetuneDatasetWithSlice(EmbeddingQAFinetuneDataset):\n",
        "\n",
        "  def _sliced(self, iterable, size):\n",
        "        iterator = iter(iterable)\n",
        "        i = 0\n",
        "        while i < len(iterable):\n",
        "          yield iterable[i:i+size]\n",
        "          i += size\n",
        "\n",
        "  def get_slices(self, limit) -> List[\"EmbeddingQAFinetuneDataset\"]:\n",
        "    \"\"\"\n",
        "    Returns a list of slices of the dataset.\n",
        "    \"\"\"\n",
        "    dataset_list = []\n",
        "    self._limit = limit\n",
        "    # Number of chunks\n",
        "    num_chunks = len(self.corpus)\n",
        "    # Number of queries, relevant docs\n",
        "    num_queries = len(self.queries)\n",
        "    num_relevant_docs = len(self.relevant_docs)\n",
        "\n",
        "    # Scale factor\n",
        "    scale_factor = num_queries // num_chunks\n",
        "\n",
        "    # Perform slicing, same length\n",
        "    sliced_queries = self._sliced(list(self.queries.items()), self._limit)\n",
        "    sliced_relevant_docs = self._sliced(list(self.relevant_docs.items()), self._limit)\n",
        "    sliced_corpus = self._sliced(list(self.corpus.items()), self._limit//scale_factor) # Length is reduced with scale\n",
        "\n",
        "    for queries, corpus, relevant_docs in zip(sliced_queries, sliced_corpus, sliced_relevant_docs):\n",
        "\n",
        "      dataset_list.append(\n",
        "          EmbeddingQAFinetuneDataset(\n",
        "              queries=dict(queries),\n",
        "              corpus=dict(corpus),\n",
        "              relevant_docs=dict(relevant_docs),\n",
        "          )\n",
        "      )\n",
        "\n",
        "    return dataset_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIBTnZzlkKhb"
      },
      "outputs": [],
      "source": [
        "qa_dataset_v1_slice = EmbeddingQAFinetuneDatasetWithSlice.from_json(\"qa_dataset_all.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oaKzLmnth37"
      },
      "outputs": [],
      "source": [
        "dataset_slices = qa_dataset_v1_slice.get_slices(limit=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GMlw-_rDHvh"
      },
      "source": [
        "The class implemented above slices the data for rate limiting and still be useful.\n",
        "\n",
        "Let's extend this class with additional embedding lookup. To avoid embedding lookups for every retrieval evaluation.\n",
        "\n",
        "But the problem is with await call of `aevaluate_dataset`. The await call is not supported with google generative ai sdk for this specific call as seen in error.\n",
        "\n",
        "To overcome this, let's implement the below workflow:\n",
        "1. Retrieve top_k docs with retriever.retrieve.\n",
        "2. Retrieved docs id is from docstore - `index.docstore.docs`.\n",
        "    - Create a lookup based on text from docstore id to node id\n",
        "    - node id is used in qa_dataset.corpus and qa_dataset.relevant_docs.\n",
        "3. Call metrics directly, it requires 5 parameters:\n",
        "  * query(from QA dataset).queries\n",
        "  * expected_ids(from QA dataset).relevant_docs[query_id]\n",
        "  * expected_texts(from QA dataset)corpus[relevant_docs_ids]\n",
        "  * retrieved_ids(from embedding retrieval) - use lookup\n",
        "  * retrieved_texts(from QA dataset)(from embedding retrieval) - use lookup\n",
        "4. `retriever.retrieve` Accepts `QueryBundle`.\n",
        "    - With QueryBundle, we can pass embeddings as well.\n",
        "    - With this setup, we can avoid creating query embeddings multiple times for different top_k values or experiments.\n",
        "\n",
        "Inputs:\n",
        "1. Custom Retriever\n",
        "2. Index(To create lookup)\n",
        "3. Query Embeddings Lookup\n",
        "4. EmbeddingsQAFineTuneDataset - This dataset hold[queries, corpus, relevant_docs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "4EcL1MOGJBMt",
        "outputId": "8a596a11-87e7-4c2b-b652-4bb97d2b201b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>llama_index.core.indices.vector_store.base.VectorStoreIndex</b><br/>def __init__(nodes: Optional[Sequence[BaseNode]]=None, use_async: bool=False, store_nodes_override: bool=False, embed_model: Optional[EmbedType]=None, insert_batch_size: int=2048, objects: Optional[Sequence[IndexNode]]=None, index_struct: Optional[IndexDict]=None, storage_context: Optional[StorageContext]=None, callback_manager: Optional[CallbackManager]=None, transformations: Optional[List[TransformComponent]]=None, show_progress: bool=False, **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/llama_index/core/indices/vector_store/base.py</a>Vector Store Index.\n",
              "\n",
              "Args:\n",
              "    use_async (bool): Whether to use asynchronous calls. Defaults to False.\n",
              "    show_progress (bool): Whether to show tqdm progress bars. Defaults to False.\n",
              "    store_nodes_override (bool): set to True to always store Node objects in index\n",
              "        store and document store even if vector store keeps text. Defaults to False</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 36);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "llama_index.core.indices.vector_store.base.VectorStoreIndex"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n_FaIXI_GQIs"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.evaluation.retrieval.metrics_base import BaseRetrievalMetric\n",
        "from llama_index.core.indices.base_retriever import BaseRetriever\n",
        "from llama_index.core.evaluation.retrieval.base import BaseRetrievalEvaluator, RetrievalEvalMode\n",
        "from llama_index.core.schema import QueryBundle\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "class Evaluator:\n",
        "  def __init__(self, index, metrics, dataset):\n",
        "    self.index: VectorStoreIndex = index\n",
        "    self.metrics: List[BaseRetrievalMetric] = metrics\n",
        "    self.dataset: EmbeddingQAFinetuneDataset = dataset\n",
        "    self.query_bundle: Dict[str, QueryBundle] = {}\n",
        "\n",
        "  def _create_query_bundle(self):\n",
        "    \"\"\"\n",
        "    Create query bundle to perform retrieval\n",
        "    \"\"\"\n",
        "\n",
        "    EMBEDDINGS_PATH = os.path.join(EXP_DIR, \"query_bundle.pkl\")\n",
        "    if os.path.exists(EMBEDDINGS_PATH):\n",
        "      self.query_bundle = pickle.load(open(EMBEDDINGS_PATH, \"rb\"))\n",
        "      return self.query_bundle\n",
        "\n",
        "    for query_id, query in self.dataset.queries.items():\n",
        "      self.query_bundle[query_id] = QueryBundle(query_str=query, embedding=embed_model_gemini.get_query_embedding(query))\n",
        "\n",
        "    pickle.dump(eval.query_bundle, open(os.path.join(EXP_DIR, \"query_bundle.pkl\")), \"wb\")\n",
        "    return self.query_bundle\n",
        "\n",
        "\n",
        "  def create_docstoreid_to_nodeid(self):\n",
        "      self.doc_id_to_node_id = {}\n",
        "      text_to_docid = {node.text: id_ for id_, node in self.index.docstore.docs.items()}\n",
        "      for node_id, text in self.dataset.corpus.items():\n",
        "        self.doc_id_to_node_id[text_to_docid[text]] = node_id\n",
        "      return self.doc_id_to_node_id\n",
        "\n",
        "  def _evaluate(self):\n",
        "      id_lookup = self.create_docstoreid_to_nodeid()\n",
        "      retrieved_ids = []\n",
        "      retrieved_texts = []\n",
        "      expected_ids = []\n",
        "      expected_texts = []\n",
        "      queries = []\n",
        "\n",
        "      for query_id, query_bundle in self.query_bundle.items():\n",
        "        retrieved_nodes = self.retriever.retrieve(query_bundle)\n",
        "        queries.append(query_bundle.query_str)\n",
        "        retrieved_ids.append([self.doc_id_to_node_id[node.node_id] for node in retrieved_nodes])\n",
        "        retrieved_texts.append([node.text for node in retrieved_nodes])\n",
        "        expected_ids.append(self.dataset.relevant_docs[query_id])\n",
        "        expected_texts.append([self.dataset.corpus[id_] for id_ in self.dataset.relevant_docs[query_id]])\n",
        "\n",
        "      return queries, retrieved_ids, retrieved_texts, expected_ids, expected_texts\n",
        "\n",
        "  def evaluate(self, top_k=[1]):\n",
        "    \"\"\"\n",
        "    By default k value is [1]\n",
        "    \"\"\"\n",
        "    self._create_query_bundle()\n",
        "    self.docstore_id_to_node_id = self.create_docstoreid_to_nodeid()\n",
        "    results = []\n",
        "    # perform retrieval\n",
        "    for k in top_k:\n",
        "      results_dict = {}\n",
        "      print(f\"Running evaluation for k-{k}\")\n",
        "      self.retriever = self.index.as_retriever(similarity_top_k=k)\n",
        "      queries, retrieved_ids, retrieved_texts, expected_ids, expected_texts = self._evaluate()\n",
        "      results_dict[\"k\"] = k\n",
        "      for query, retrieved_id, retrieved_text, expected_id, expected_text in zip(queries, retrieved_ids, retrieved_texts, expected_ids, expected_texts):\n",
        "        results_dict[\"query\"] = query\n",
        "        results_dict[\"expected_ids\"] = expected_id\n",
        "        results_dict[\"expected_texts\"] = expected_text\n",
        "        results_dict[\"retrieved_ids\"] = retrieved_id\n",
        "        results_dict[\"retrieved_texts\"] = retrieved_text\n",
        "        for metric in self.metrics:\n",
        "          results_dict[metric.metric_name] = metric.compute(\n",
        "              query=query,\n",
        "              expected_ids=expected_id,\n",
        "              expected_texts=expected_text,\n",
        "              retrieved_ids=retrieved_id,\n",
        "              retrieved_texts=retrieved_text,\n",
        "          ).score\n",
        "        results.append(results_dict)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CffXgBKHQyha"
      },
      "outputs": [],
      "source": [
        "eval = Evaluator(\n",
        "    index=index,\n",
        "    metrics=[hit_rate, mrr],\n",
        "    dataset=qa_dataset_v1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bOAqWF00b5V",
        "outputId": "58b6923d-f02b-41a4-aa56-979a14ffa0d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running evaluation for k-3\n"
          ]
        }
      ],
      "source": [
        "results = eval.evaluate(top_k=[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zuq1KXmh0fpc",
        "outputId": "a60efed9-0ec1-4ccb-e242-0e27f8c3705b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['5a905be5-61e3-4271-a74d-5dfbc586c59b',\n",
              " '5b327843-43d7-4e1c-bb7c-06139c1d4b6a',\n",
              " 'fe147fc4-7b27-4378-b100-441c8b92984b']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(results).iloc[0][\"retrieved_ids\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRlq1OheRXW_",
        "outputId": "0257637c-0ca3-4ced-f1aa-3522d3b3548d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running evaluation for k-1\n",
            "Running evaluation for k-2\n",
            "Running evaluation for k-3\n",
            "Running evaluation for k-4\n",
            "Running evaluation for k-5\n"
          ]
        }
      ],
      "source": [
        "results = eval.evaluate(top_k=[1,2,3,4,5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMbtr5IHAstG"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Fnf8F_2EDaS1",
        "outputId": "797f3c66-1895-4f06-8e53-a86c502429d0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"k\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          2,\n          5,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hit_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mrr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-7985d9f7-7a9c-4691-a138-0b77933bb2be\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hit_rate</th>\n",
              "      <th>mrr</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>k</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7985d9f7-7a9c-4691-a138-0b77933bb2be')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7985d9f7-7a9c-4691-a138-0b77933bb2be button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7985d9f7-7a9c-4691-a138-0b77933bb2be');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-442acef5-ec38-4997-8350-1bdb08a92068\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-442acef5-ec38-4997-8350-1bdb08a92068')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-442acef5-ec38-4997-8350-1bdb08a92068 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   hit_rate  mrr\n",
              "k               \n",
              "1       1.0  1.0\n",
              "2       1.0  1.0\n",
              "3       1.0  1.0\n",
              "4       1.0  1.0\n",
              "5       1.0  1.0"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get mean hit rates of k\n",
        "results_df.groupby(\"k\")[[\"hit_rate\", \"mrr\"]].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drO2HXDfUi5L"
      },
      "source": [
        "With the changes[Embedding Model, QA dataset Generation LLM, Chunk size]. We've went from hit_rate and mrr of 0.0 to 1.0(This is fishy).\n",
        "\n",
        "TODO: As a system, we can scriptize the code above for creation, loading, evaluation process.\n",
        "\n",
        "TODO: We can incorprate a reranker as postprocessor to work with retriever to improve the hit rate and mrr. Since hit rate's already 1.0, let's skip this.\n",
        "\n",
        "Syntax change is realy small, include **kwargs or param for evaluator and update retriever creation as follows:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from llama_index.core.postprocessor import SentenceTransformerRerank,LLMRerank\n",
        "\n",
        "st_reranker = SentenceTransformerRerank(\n",
        "    top_n=5, model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
        ")\n",
        "\n",
        "llm_reranker = LLMRerank(\n",
        "    choice_batch_size=4, top_n=5,\n",
        ")\n",
        "cohere_rerank = CohereRerank(api_key=os.getenv('COHERE_API_KEY'), top_n=10)\n",
        "\n",
        "index.as_retriever(similarity_top_k=k, postprocessor=reranker)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hbzgvj_WmnK"
      },
      "source": [
        "## Generation Evaluation\n",
        "\n",
        "The entire work on achieving a good retrieval is to ensure generated results are good for the end user. Because retrieval is a main bottlenect in a RAG system. How?\n",
        "\n",
        "* Retrieved context is the latest information passed to LLM for the query.\n",
        "* With irrelevant context, we will have bad generations.\n",
        "* With Huge Conexts, we'll cram up the LLM context space and increase the cost.\n",
        "\n",
        "With a proper retrieval evaluation setup, we can avoid above performance bottlenecks.\n",
        "\n",
        "Next, let's evaluate the Generation based on the retrieve context with two metrics using LLM as a Judge.\n",
        "\n",
        "LLM as a judge uses a prompted LLM components to evaluate the response along with context on dimensions required. Here we'll evaluate two:\n",
        "\n",
        "1. [Relevance](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/evaluation/relevancy.py): Whether retrieved context and answer are relevant to query.\n",
        "2. [Faithfulness](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/evaluation/faithfulness.py): Whether the generated answer is grounded in context without hallucination.\n",
        "\n",
        "To do this llama-index provides two evaluators. To look at the prompts take a look at the source code. The prompts here are really good sources for reference.\n",
        "\n",
        "BatchEvalRunner accepts Evaluators[RelevancyEvaluator, FaithfulnessEvaluator]. [BatchEvalRunner.aevaluate_queries()](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/evaluation/batch_runner.py#L261) calls asynchronous workers and obtaines responses for the queries. These queries, responses are then passed on to Evaluators to evaluate the responses with a prompted LLM component."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "TF7jrMmkXw55",
        "outputId": "a2dfdcfb-6937-4e35-b48b-1625e6c89be3"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "asyncio.run() cannot be called from a running event loop",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-c137b051f870>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   )\n\u001b[1;32m     16\u001b[0m   \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_dataset_v1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   eval_result = asyncio.run(runner.aevaluate_queries(\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mquery_engine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   ))\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/runners.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# fail fast with short traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    187\u001b[0m             \"asyncio.run() cannot be called from a running event loop\")\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
          ]
        }
      ],
      "source": [
        "from llama_index.core.evaluation import RelevancyEvaluator, FaithfulnessEvaluator, BatchEvalRunner\n",
        "import asyncio\n",
        "\n",
        "faithfullness_evaluator = FaithfulnessEvaluator(llm=llm_gemini)\n",
        "relevancy_evaluator = RelevancyEvaluator(llm=llm_gemini)\n",
        "gen_eval_results = []\n",
        "runner = BatchEvalRunner(\n",
        "    evaluators={\"faithfulness\": FaithfulnessEvaluator, \"relevancy\": RelevancyEvaluator},\n",
        "    show_progress=True,\n",
        ")\n",
        "\n",
        "for k in [1,2,3,4,5]:\n",
        "  query_engine = index.as_query_engine(\n",
        "      similarity_top_k=k\n",
        "  )\n",
        "  queries = list(qa_dataset_v1.queries.values())[:30]\n",
        "  eval_result = asyncio.run(runner.aevaluate_queries(\n",
        "      query_engine, queries=queries,\n",
        "  ))\n",
        "  gen_eval_results.append(eval_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKFfJ-SuXwaZ"
      },
      "outputs": [],
      "source": [
        "qe = index.as_query_engine(similarity_top_k=1)\n",
        "response = qe.query(\"What did paul graham do?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "zowziapmz9ox",
        "outputId": "474559b9-45be-4ca1-a841-3d999c96c3ee"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Now anyone could publish anything.\\n\\nThis had been possible in principle since 1993, but not many people had realized it yet. I had been intimately involved with building the infrastructure of the web for most of that time, and a writer as well, and it had taken me 8 years to realize it. Even then it took me several years to understand the implications. It meant there would be a whole new generation of essays. [11]\\n\\nIn the print era, the channel for publishing essays had been vanishingly small. Except for a few officially anointed thinkers who went to the right parties in New York, the only people allowed to publish essays were specialists writing about their specialties. There were so many essays that had never been written, because there had been no way to publish them. Now they could be, and I was going to write them. [12]\\n\\nI've worked on several different things, but to the extent there was a turning point where I figured out what to work on, it was when I started publishing essays online. From then on I knew that whatever else I did, I'd always write essays too.\""
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.source_nodes[0].get_content()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tNVVEadRPIAZ"
      },
      "outputs": [],
      "source": [
        "# load index from disk\n",
        "EXP_INDEX_DIR = os.path.join(EXP_DIR, \"index\")\n",
        "Settings.llm = llm_gemini\n",
        "Settings.embed_model = embed_model_gemini\n",
        "vector_store = FaissVectorStore.from_persist_dir(EXP_INDEX_DIR)\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=vector_store, persist_dir=EXP_INDEX_DIR,\n",
        ")\n",
        "index = load_index_from_storage(storage_context=storage_context)\n",
        "\n",
        "qa_dataset_path = os.path.join(EXP_DIR, \"qa_dataset_all.json\")\n",
        "qa_dataset_v1 = EmbeddingQAFinetuneDataset.from_json(qa_dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HcGEgIqGEx3l"
      },
      "outputs": [],
      "source": [
        "# Prompts\n",
        "# Source https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/evaluation/faithfulness.py#L95\n",
        "# https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/evaluation/relevancy.py\n",
        "\n",
        "from llama_index.core.prompts import PromptTemplate\n",
        "\n",
        "# Faithfulness Templates\n",
        "FAITHFULNESS_EVAL_TEMPLATE = PromptTemplate(\n",
        "    \"Please tell if a given piece of information \"\n",
        "    \"is supported by the context.\\n\"\n",
        "    \"You need to answer with either YES or NO.\\n\"\n",
        "    \"Answer YES if any of the context supports the information, even \"\n",
        "    \"if most of the context is unrelated. \"\n",
        "    \"Some examples are provided below. \\n\\n\"\n",
        "    \"Information: Apple pie is generally double-crusted.\\n\"\n",
        "    \"Context: An apple pie is a fruit pie in which the principal filling \"\n",
        "    \"ingredient is apples. \\n\"\n",
        "    \"Apple pie is often served with whipped cream, ice cream \"\n",
        "    \"('apple pie à la mode'), custard or cheddar cheese.\\n\"\n",
        "    \"It is generally double-crusted, with pastry both above \"\n",
        "    \"and below the filling; the upper crust may be solid or \"\n",
        "    \"latticed (woven of crosswise strips).\\n\"\n",
        "    \"Answer: YES\\n\"\n",
        "    \"Information: Apple pies tastes bad.\\n\"\n",
        "    \"Context: An apple pie is a fruit pie in which the principal filling \"\n",
        "    \"ingredient is apples. \\n\"\n",
        "    \"Apple pie is often served with whipped cream, ice cream \"\n",
        "    \"('apple pie à la mode'), custard or cheddar cheese.\\n\"\n",
        "    \"It is generally double-crusted, with pastry both above \"\n",
        "    \"and below the filling; the upper crust may be solid or \"\n",
        "    \"latticed (woven of crosswise strips).\\n\"\n",
        "    \"Answer: NO\\n\"\n",
        "    \"Information: {query_str}\\n\"\n",
        "    \"Context: {context_str}\\n\"\n",
        "    \"Answer: \"\n",
        ")\n",
        "\n",
        "FAITHFULNESS_REFINE_TEMPLATE = PromptTemplate(\n",
        "    \"We want to understand if the following information is present \"\n",
        "    \"in the context information: {query_str}\\n\"\n",
        "    \"We have provided an existing YES/NO answer: {existing_answer}\\n\"\n",
        "    \"We have the opportunity to refine the existing answer \"\n",
        "    \"(only if needed) with some more context below.\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"{context_msg}\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"If the existing answer was already YES, still answer YES. \"\n",
        "    \"If the information is present in the new context, answer YES. \"\n",
        "    \"Otherwise answer NO.\\n\"\n",
        ")\n",
        "\n",
        "# Relevancy templates\n",
        "RELEVANCY_EVAL_TEMPLATE = PromptTemplate(\n",
        "    \"Your task is to evaluate if the response for the query \\\n",
        "    is in line with the context information provided.\\n\"\n",
        "    \"You have two options to answer. Either YES/ NO.\\n\"\n",
        "    \"Answer - YES, if the response for the query \\\n",
        "    is in line with context information otherwise NO.\\n\"\n",
        "    \"Query and Response: \\n {query_str}\\n\"\n",
        "    \"Context: \\n {context_str}\\n\"\n",
        "    \"Answer: \"\n",
        ")\n",
        "\n",
        "RELEVANCY_REFINE_TEMPLATE = PromptTemplate(\n",
        "    \"We want to understand if the following query and response is\"\n",
        "    \"in line with the context information: \\n {query_str}\\n\"\n",
        "    \"We have provided an existing YES/NO answer: \\n {existing_answer}\\n\"\n",
        "    \"We have the opportunity to refine the existing answer \"\n",
        "    \"(only if needed) with some more context below.\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"{context_msg}\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"If the existing answer was already YES, still answer YES. \"\n",
        "    \"If the information is present in the new context, answer YES. \"\n",
        "    \"Otherwise answer NO.\\n\"\n",
        ")\n",
        "\n",
        "# Completion prompt\n",
        "qa_prompt = PromptTemplate(\n",
        "    \"\"\"\\\n",
        "Context information is below.\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {query_str}\n",
        "Answer: \\\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "f8I7j6-XJ2_G"
      },
      "outputs": [],
      "source": [
        "from google.api_core.exceptions import TooManyRequests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "qUwXE_3cJ5uJ",
        "outputId": "d208a965-b6b1-4d9b-fb0b-a01f5c5c345c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "google.api_core.exceptions.TooManyRequests"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>google.api_core.exceptions.TooManyRequests</b><br/>def __init__(message, errors=(), details=(), response=None, error_info=None)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/google/api_core/exceptions.py</a>Exception mapping a ``429 Too Many Requests`` response.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 366);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "TooManyRequests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "jY8WMqh36Y7x"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from tqdm.contrib import tzip\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "class EvaluateResponse:\n",
        "\n",
        "  def __init__(self, llm, index, dataset, k=1, show_progress=True, slice_range: Optional[Tuple[int: int]] = None):\n",
        "    self.llm = llm or Settings.llm\n",
        "    self.index: VectorStoreIndex = index\n",
        "    self.dataset: EmbeddingQAFinetuneDataset = dataset\n",
        "    self.retriever = index.as_retriever(similarity_top_k=k)\n",
        "    self.show_progress = show_progress\n",
        "\n",
        "    if slice_range:\n",
        "          start, end = slice_range\n",
        "          # Validate slice range\n",
        "          start = max(0, start)  # Ensure start is not negative\n",
        "          end = min(len(self.dataset.queries), end)  # Ensure end does not exceed length\n",
        "\n",
        "          # Slice queries, relevant_docs, and corpus dictionaries\n",
        "          self.dataset.queries = dict(list(self.dataset.queries.items())[start:end])\n",
        "          self.dataset.relevant_docs = dict(list(self.dataset.relevant_docs.items())[start:end])\n",
        "          self.dataset.corpus = dict(list(self.dataset.corpus.items())[start:end])\n",
        "\n",
        "    self.eval_params = {\n",
        "        \"queries\": {},\n",
        "        \"query_embeddings\": {},\n",
        "        \"retrieved_text\": {},\n",
        "        \"response\": {},\n",
        "    }\n",
        "    self.eval_results = []\n",
        "    # self._query_embeddings()\n",
        "    # self._retrieve()\n",
        "    # self._generate_response()\n",
        "\n",
        "  @retry(\n",
        "      stop=stop_after_attempt(5),\n",
        "      wait=wait_exponential(multiplier=1, min=4, max=10),\n",
        "      retry=(retry_if_exception_type(TooManyRequests)),\n",
        "  )\n",
        "  def _query_embeddings(self, sleep=None):\n",
        "    self.eval_params[\"queries\"] = self.dataset.queries\n",
        "    self.query_embeddings = {}\n",
        "    for query_id, query in tqdm(self.dataset.queries.items(), disable=not self.show_progress, desc=\"Generating query embeddings\"):\n",
        "      if query_id in self.eval_params[\"query_embeddings\"]:\n",
        "        continue\n",
        "      self.eval_params[\"query_embeddings\"].update({query_id: embed_model_gemini.get_query_embedding(query)})\n",
        "\n",
        "    return self.eval_params[\"query_embeddings\"]\n",
        "\n",
        "  def _retrieve(self, k=2):\n",
        "    retreiver = self.index.as_retriever(similarity_top_k=k)\n",
        "    # Loop through queries and obtain retrieved docs\n",
        "    for query_id in tqdm(self.eval_params[\"queries\"].keys(), disable=not self.show_progress, desc=\"Retrieve context\"):\n",
        "      retrieved_nodes = retreiver.retrieve(\n",
        "          QueryBundle(\n",
        "              query_str=self.eval_params[\"queries\"][query_id],\n",
        "              embedding=self.eval_params[\"query_embeddings\"][query_id],\n",
        "          )\n",
        "      )\n",
        "      self.eval_params[\"retrieved_text\"][query_id] = [node.get_content() for node in retrieved_nodes]\n",
        "    return self.eval_params[\"retrieved_text\"]\n",
        "\n",
        "  @retry(\n",
        "      stop=stop_after_attempt(5),\n",
        "      wait=wait_exponential(multiplier=1, min=4, max=10),\n",
        "      retry=(retry_if_exception_type(TooManyRequests)),\n",
        "  )\n",
        "  def _generate_response(self):\n",
        "    for query_id in tqdm(self.eval_params[\"queries\"].keys(), disable=not self.show_progress, desc=\"Generating Response\"):\n",
        "      context_str = \"\\n\\n\".join(self.eval_params[\"retrieved_text\"][query_id])\n",
        "      # format query prompt\n",
        "      fmt_qa_prompt = qa_prompt.format(context_str=context_str, query_str=self.eval_params[\"queries\"][query_id])\n",
        "      if query_id in self.eval_params[\"response\"]:\n",
        "        continue\n",
        "      # generate response\n",
        "      response = self.llm.complete(fmt_qa_prompt)\n",
        "      self.eval_params[\"response\"][query_id] = response\n",
        "\n",
        "  def evaluate(self):\n",
        "\n",
        "    # We'll perform evaluation with default templates.\n",
        "    # TODO for refine https://docs.llamaindex.ai/en/stable/examples/low_level/response_synthesis/#2-try-a-create-and-refine-strategy\n",
        "    # TODO: Make it concurrent with respect to workers to speed up eval.\n",
        "    for query_id, query_str, context_str, response_str in tzip(\n",
        "        self.eval_params[\"queries\"].keys(),\n",
        "        self.eval_params[\"queries\"].values(),\n",
        "        self.eval_params[\"retrieved_text\"].values(),\n",
        "        self.eval_params[\"response\"].values(),\n",
        "        desc=\"Evaluating Responses\",\n",
        "        disable=not self.show_progress,\n",
        "    ):\n",
        "\n",
        "      fa_fmt_prompt = FAITHFULNESS_EVAL_TEMPLATE.format(query_str=query_str, context_str=\"\\n\\n\".join(context_str))\n",
        "      relevancy_fmt_prompt = RELEVANCY_EVAL_TEMPLATE.format(query_str=query_str, context_str=\"\\n\\n\".join(context_str))\n",
        "\n",
        "      faithfulness_response = self.llm.complete(fa_fmt_prompt)\n",
        "      relevancy_response = self.llm.complete(relevancy_fmt_prompt)\n",
        "\n",
        "      if \"yes\" in faithfulness_response.text.lower():\n",
        "        faithfulness = True\n",
        "      else:\n",
        "        faithfulness = False\n",
        "\n",
        "      if \"yes\" in relevancy_response.text.lower():\n",
        "        relevancy = True\n",
        "      else:\n",
        "        relevancy = False\n",
        "\n",
        "      self.eval_results.append({\n",
        "          \"query_id\": query_id,\n",
        "          \"query\": query_str,\n",
        "          \"context\": context_str,\n",
        "          \"response\": response_str,\n",
        "          \"failthfulness_response\": faithfulness_response.text,\n",
        "          \"relevancy_response\": relevancy_response.text,\n",
        "          \"faithfulness\": int(faithfulness),\n",
        "          \"relevancy\": int(relevancy),\n",
        "      })\n",
        "\n",
        "    return self.eval_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "tDZ3K34-bdhl"
      },
      "outputs": [],
      "source": [
        "eval_response_1 = EvaluateResponse(\n",
        "    llm=llm_gemini,\n",
        "    index=index,\n",
        "    dataset=qa_dataset_v1,\n",
        "    k=1,\n",
        "    show_progress=True,\n",
        "    slice_range=(0,5)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings, perfrm retrieval, generate responses and assign them to variables\n",
        "from llama_index.core.schema import QueryBundle\n",
        "embeddings = eval_response_1._query_embeddings()\n",
        "retrievals = eval_response_1._retrieve()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "ExCdqmiI8NMx",
        "outputId": "a5705fee-92b9-4699-e03a-e12c5be486d6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating query embeddings: 100%|██████████| 5/5 [00:09<00:00,  1.84s/it]\n",
            "Retrieve context: 100%|██████████| 5/5 [00:00<00:00, 838.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "etU3TUPNIa-l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cd81117-0334-4d5e-8734-010e569b3779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Response: 100%|██████████| 5/5 [00:09<00:00,  1.98s/it]\n"
          ]
        }
      ],
      "source": [
        "respones = eval_response_1._generate_response()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = eval_response_1.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e99b7c0daad14d52965897c3af576366",
            "bd48da20443742b686be328f29a59aa6",
            "2f1acae64d6e4959b3077f56b111245d",
            "5e07e3508328484ea0bac938e454eb3c",
            "c2db79d42bea4bf1a675a8c632a7dd6e",
            "5a18e058f93f40129a55640153c131fd",
            "dbee6ae1e6ba4bfaa6ed68f481d7b6a9",
            "c915c3221aed4a90a64633e44fabf165",
            "d516d0e2662c4c4ab8ee09dae263a4ba",
            "ac3a8a2cbda04c05843e4f708fb6e54d",
            "167a523a919e40deaa5275a19fd40395"
          ]
        },
        "id": "UVJYFWO994oA",
        "outputId": "2fda1c46-93e3-4616-bf94-dc49e7941baa"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating Responses:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e99b7c0daad14d52965897c3af576366"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(eval_results).T.faithfulness.astype(int).mean(), pd.DataFrame(eval_results).T.relevancy.astype(int).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkUEe1vq_xvR",
        "outputId": "06cd4d7d-b757-4a32-c42e-f70a05941cc1"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running eval on only 5 queries due to free quota resource limits. :|"
      ],
      "metadata": {
        "id": "2Q4EQMkrBntO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXU-9NgCgouq"
      },
      "source": [
        "This concludes the end of the notebook.\n",
        "\n",
        "In this we've covered below topic and their under the hood workings with llama-index:\n",
        "1. Index Creation/Store/Load\n",
        "2. Retriever Evaluation(Building Retriever Evaluator, Using Metrics with granualar evaluation)\n",
        "3. Generation Evaluation(Building Response Evaluator)\n",
        "\n",
        "These code blocks are built to overcome BatchEmbedContent rate limits with free resouce quota usage with Gemini API.\n",
        "\n",
        "With this building, under the hood workings of evaluators, metrics in llama-index are covered in this notebook."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00c66838a1a948cca84b86a3040e0030": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12840615d2e445c29032fee20d9a404f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23570a4a0ace43368344754cf68e827b",
              "IPY_MODEL_52c68508da7242fb838f1f12be366cb6",
              "IPY_MODEL_253880f2060546da8773033e9451b124"
            ],
            "layout": "IPY_MODEL_9ea9907194a54cef9273bdb1fd3ffd9d"
          }
        },
        "12c0faf254aa46f5b4c15967b66cf6bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14c2cf595be74112998e66d035eaaae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17a7a7126cfe40a9b1e3f8ebd6a1daba",
              "IPY_MODEL_ff6da244afd340fd81187f82e259472f",
              "IPY_MODEL_2050d6f1d9164657a14973c625092670"
            ],
            "layout": "IPY_MODEL_e581e99ee084419295ab52b77757c6c7"
          }
        },
        "17a7a7126cfe40a9b1e3f8ebd6a1daba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9140c0a56591430799f54521bb6b4338",
            "placeholder": "​",
            "style": "IPY_MODEL_88b5a644b77a46ea95f6646621a26d2f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2050d6f1d9164657a14973c625092670": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64ce9dcea9d94f9ca27d8842f67850e2",
            "placeholder": "​",
            "style": "IPY_MODEL_d26cd61a697d42d5857c088c222c9b98",
            "value": " 2/2 [00:03&lt;00:00,  1.66s/it]"
          }
        },
        "209b3bb6278e4848b8b981b70c55d75a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00c66838a1a948cca84b86a3040e0030",
            "max": 37,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12c0faf254aa46f5b4c15967b66cf6bb",
            "value": 37
          }
        },
        "2198d5ab1ba94d9fbd72c2995e8b8a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23570a4a0ace43368344754cf68e827b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af15b3eb95d1476084bdda3c5c37f8db",
            "placeholder": "​",
            "style": "IPY_MODEL_2198d5ab1ba94d9fbd72c2995e8b8a89",
            "value": "Generating embeddings: 100%"
          }
        },
        "253880f2060546da8773033e9451b124": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e75081e2edcd42e7b504501f9de2f340",
            "placeholder": "​",
            "style": "IPY_MODEL_daed53877c8845b18f06ac0438cb67e0",
            "value": " 83/83 [01:45&lt;00:00,  1.37s/it]"
          }
        },
        "329f46150ee840d691d2a0bce9d24d6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e64b0e4632d42ceb1e94b2c35143143": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52c68508da7242fb838f1f12be366cb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1d69ff3d7574328b4a2b59303cda3d5",
            "max": 83,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3815231cff044f595445fc03dd1030a",
            "value": 83
          }
        },
        "64ce9dcea9d94f9ca27d8842f67850e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88b5a644b77a46ea95f6646621a26d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f5a326570ca4f099f8362297810ead6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9140c0a56591430799f54521bb6b4338": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98d9afff57044c0e9e89ed2c359cf7d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ea9907194a54cef9273bdb1fd3ffd9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af15b3eb95d1476084bdda3c5c37f8db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0187ea479de46588f5e9c98b8691987": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_329f46150ee840d691d2a0bce9d24d6b",
            "placeholder": "​",
            "style": "IPY_MODEL_98d9afff57044c0e9e89ed2c359cf7d5",
            "value": "Generating embeddings: 100%"
          }
        },
        "b0a56d0fd46f4fb88c7ac073516ffd50": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bba2b8778f7940b0853f9a33efcf947d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0187ea479de46588f5e9c98b8691987",
              "IPY_MODEL_209b3bb6278e4848b8b981b70c55d75a",
              "IPY_MODEL_f8b67a2eb2d44ef28ceae2d0de6f66ad"
            ],
            "layout": "IPY_MODEL_b0a56d0fd46f4fb88c7ac073516ffd50"
          }
        },
        "d26cd61a697d42d5857c088c222c9b98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daed53877c8845b18f06ac0438cb67e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1d69ff3d7574328b4a2b59303cda3d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e581e99ee084419295ab52b77757c6c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e75081e2edcd42e7b504501f9de2f340": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e89cb216150c4bbe9a8445f8be61c39e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3815231cff044f595445fc03dd1030a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8367cd7025648ee85e961c9548e9444": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8b67a2eb2d44ef28ceae2d0de6f66ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8367cd7025648ee85e961c9548e9444",
            "placeholder": "​",
            "style": "IPY_MODEL_4e64b0e4632d42ceb1e94b2c35143143",
            "value": " 37/37 [16:36&lt;00:00, 26.72s/it]"
          }
        },
        "ff6da244afd340fd81187f82e259472f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e89cb216150c4bbe9a8445f8be61c39e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f5a326570ca4f099f8362297810ead6",
            "value": 2
          }
        },
        "e99b7c0daad14d52965897c3af576366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd48da20443742b686be328f29a59aa6",
              "IPY_MODEL_2f1acae64d6e4959b3077f56b111245d",
              "IPY_MODEL_5e07e3508328484ea0bac938e454eb3c"
            ],
            "layout": "IPY_MODEL_c2db79d42bea4bf1a675a8c632a7dd6e"
          }
        },
        "bd48da20443742b686be328f29a59aa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a18e058f93f40129a55640153c131fd",
            "placeholder": "​",
            "style": "IPY_MODEL_dbee6ae1e6ba4bfaa6ed68f481d7b6a9",
            "value": "Evaluating Responses: 100%"
          }
        },
        "2f1acae64d6e4959b3077f56b111245d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c915c3221aed4a90a64633e44fabf165",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d516d0e2662c4c4ab8ee09dae263a4ba",
            "value": 5
          }
        },
        "5e07e3508328484ea0bac938e454eb3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac3a8a2cbda04c05843e4f708fb6e54d",
            "placeholder": "​",
            "style": "IPY_MODEL_167a523a919e40deaa5275a19fd40395",
            "value": " 5/5 [00:16&lt;00:00,  3.17s/it]"
          }
        },
        "c2db79d42bea4bf1a675a8c632a7dd6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a18e058f93f40129a55640153c131fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbee6ae1e6ba4bfaa6ed68f481d7b6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c915c3221aed4a90a64633e44fabf165": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d516d0e2662c4c4ab8ee09dae263a4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac3a8a2cbda04c05843e4f708fb6e54d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "167a523a919e40deaa5275a19fd40395": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}